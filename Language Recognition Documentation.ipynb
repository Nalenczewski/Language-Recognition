{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d277f54c",
   "metadata": {},
   "source": [
    "![](./README_files/flag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c4625",
   "metadata": {},
   "source": [
    "# Language Recognition and First Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b013102",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. **Introduction**\n",
    "2. **My Approach**\n",
    "3. **Findings**\n",
    "4. **Ideas for Further Research**\n",
    "5. **Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800498c3",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Today, we take it for granted that Google can automatically detect a what language a given text is written in and then translate the text with a high degree of accuracy. The goal of my model is to recognize what language a given text is written in and output the language name.\n",
    "\n",
    "I used a [language identification dataset from Kaggle](https://www.kaggle.com/datasets/zarajamshaid/language-identification-datasst) that comprises 22,000 paragraphs of 22 languages and is taken from the original WiLI-2018 Wikipedia language identification benchmark dataset that contians 235,000 paragraphs of 235 langauges. The languages in the dataset I used are:\n",
    "* English\n",
    "* Arabic\n",
    "* French\n",
    "* Hindi\n",
    "* Urdu\n",
    "* Portuguese\n",
    "* Persian\n",
    "* Pushto\n",
    "* Spanish\n",
    "* Korean\n",
    "* Tamil\n",
    "* Turkish\n",
    "* Estonian\n",
    "* Russian\n",
    "* Romanian\n",
    "* Chinese\n",
    "* Swedish\n",
    "* Latin\n",
    "* Indonesian\n",
    "* Dutch\n",
    "* Japanese\n",
    "* Thai\n",
    "\n",
    "The data consist of two columns: a natural language text and a categorial label. The first few lines of the data are below.\n",
    "![](./README_files/ex1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdc63a",
   "metadata": {},
   "source": [
    "The data contained 1,000 examples of each language for a total of 22,000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c150c",
   "metadata": {},
   "source": [
    "A secondary goal of this project is building a model that can predict whether a sentence is the first in a paragraph or not. It will take document vectors taken created by a SVD transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ffa9c7",
   "metadata": {},
   "source": [
    "## 2. My Approach\n",
    "\n",
    "**Language Recognition**\n",
    "\n",
    "I built four different models by first vectorizing the text data using Count Vectorizer and Tf-idf, and then training Logistic Regression and Naive Bayes models on the data. I then evaluated the performance of each model using accuracy score, precision, and recall.\n",
    "\n",
    "**Predicting Language of Text Taken from the Internet**\n",
    "\n",
    "I further tested each model's predictive ability by building a function that would take in a paragraph of text in one of the 22 langauges and output its label. The data was taken from various news and other sites on the internet.\n",
    "\n",
    "I built a predict function using each of the four trained models and fed each one 22 different paragraphs, one for each language, taken from the Internet, primarily news sites, in order to predict what language the paragraph was. \n",
    "\n",
    "**First Sentence Prediction**\n",
    "\n",
    "For the second half of this project, I built a model that can identify whether a sentence is the first in its paragraph or not. To do this, I first chose one language to work with, in this case, Chinese. I also chose Chinese because it was the only data which still had punctuation in it. \n",
    "\n",
    "Next, I needed to create a new self-supervised dataset containing individual sentences and a label indicating if the sentence is the first of a paragraph or not. To this end, I constructed a function that would use spaCy to create spaCy document objects from the paragraphs, split the data up into sentences using the sents method on the document object, label each sentence as the first in its paragraph or not, and construct the new dataframe containing all of the individual sentences from each paragraph and their labels.\n",
    "\n",
    "To keep the sentneces from being taken out of context during the train-test split, I first did train-test split on the individual paragraphs. Then, I applied the function to create new dataframes for training data and test data.\n",
    "\n",
    "Because I was dealing with a highly imbalanced dataset, I did random oversampling on the minority class (the first sentneces).\n",
    "\n",
    "At this point, I was ready to do a latent semantic analysis (LSA) on the data in order to create document vectors that could then be fed to a machine learning model for making predictions. To do this, I used CountVectorizer and Tf-idf to transform the sentences into a bag of words. I then used Truncated SVD with 75 components to turn the document term matrices into latent semantic analyses. \n",
    "\n",
    "Once I had the latent semantic analysis vectors (75 components), I fit a Logistic Regression model on the data, made predictions on the training and test sets, and evaluated the results using accuracy score, precision, and recall.\n",
    "\n",
    "After getting the results, I found that Tf-idf with LSA performed the best. So far, the model has been optimizing for accuracy. But in a classification problem with a miority class, it is more important to optimize for F1 score. So, I created a grid search with the scoring parameter set to f1 in order to optimize for f1 score instead of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf76781",
   "metadata": {},
   "source": [
    "## 3. Findings\n",
    "\n",
    "**Language Recognition**\n",
    "\n",
    "Below is a table showing the accuracy scores of the models. Count Vectorizer with Naive Bayes performed the best.\n",
    "![](./README_files/ex2.png)\n",
    "\n",
    "**Predicting Language of Text Taken from the Internet**\n",
    "\n",
    "Most languages were predicted correctly, however Japanese and Chinese were sometimes predicted as other languages, such as Japanese as Russian or Chinese as Japanese. Sometimes, when a prediction was made on the same language multiple times, the algorithm would predict different languages from one time to the next. I believe the reason for this is because Chinese and Japanese do not contain spaces and therefore cannot be tokenized.\n",
    "\n",
    "**First Sentence Prediction**\n",
    "\n",
    "The results of predictions of the models optimized for f1 score versus those optimized for accuracy did not acctually differ. Below are the results.\n",
    "![](./README_files/ex3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f994798",
   "metadata": {},
   "source": [
    "## 4. Ideas for Further Research\n",
    "\n",
    "**Language Recognition and Predicting Langauge of Text Taken from the Internet**\n",
    "\n",
    "* Use packages like jieba (Chinese tokenizer) and nagisa (Japanese tokenizer) to tokenize Chinese and Japanese, then combine them with the rest of the tokenized langauge data before doing the modeling.\n",
    "* Try to solve the same problem but instead of using machine learning, use language dictionaries.\n",
    "* I could expand the langauges corpus to include more languages.\n",
    "\n",
    "**First Sentence Prediction**\n",
    "\n",
    "* I could try inputting some sentences from paragraphs gathered on the Chinese web and see how well the model predicts the sentence labels.\n",
    "* Follow the same process but for Wikipedia data in English. To do this, I would need to scrape Wikipedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c5132",
   "metadata": {},
   "source": [
    "## 5. Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489297",
   "metadata": {},
   "source": [
    "**Language Recognition and Predicting Langauge of Text Taken from the Internet**\n",
    "\n",
    "The language recognition model could be incorporated into an app that recognizes a language in order to translate it correctly.\n",
    "\n",
    "**First Sentence Prediction**\n",
    "\n",
    "It could be used to build a list of most common first sentences, which could then be characterized. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
