{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538937b0",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b885e",
   "metadata": {},
   "source": [
    "Next, I will create the X and y variables for modeling. Since the y variable will be the same for all models, I will only need to create it once. However, since I am using two different methods to create two sets of feature vectors, I need to create two different Xs. I will create each X variable as I go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857af1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = df['Text']\n",
    "y = df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bdcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels into numbers.\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f0d44",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizer + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d534b",
   "metadata": {},
   "source": [
    "First I will create a Count Vectorizer Logistic Regression model. The first step is to create some sparse matrices for the X variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce62345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Count Vectorizer and set X_cv equal to the transformed data\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7d1f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6f0fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the number of tokens in the first example.\n",
    "len(set(X[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa82bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(X_cv, y, test_size = 0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0038b8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16500, 277720), (5500, 277720))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the shape of the data\n",
    "X_cv_train.shape, X_cv_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aaa6612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of CountVectorizer\n",
    "cv_lr = LogisticRegression()\n",
    "cv_lr.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9be89",
   "metadata": {},
   "source": [
    "Next, I will make a dictionary of language-label encoded numbers so that I will be able to understand the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0383e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 17, 19, 18,  2,  8, 20, 10, 21,  7, 12,  5,  1,  9,  6, 16, 13,\n",
       "       11, 14, 15,  3,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b83a985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c7d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Arabic\n",
      "1 Chinese\n",
      "2 Dutch\n",
      "3 English\n",
      "4 Estonian\n",
      "5 French\n",
      "6 Hindi\n",
      "7 Indonesian\n",
      "8 Japanese\n",
      "9 Korean\n",
      "10 Latin\n",
      "11 Persian\n",
      "12 Portugese\n",
      "13 Pushto\n",
      "14 Romanian\n",
      "15 Russian\n",
      "16 Spanish\n",
      "17 Swedish\n",
      "18 Tamil\n",
      "19 Thai\n",
      "20 Turkish\n",
      "21 Urdu\n"
     ]
    }
   ],
   "source": [
    "d = dict(zip([ 4, 17, 19, 18,  2,  8, 20, 10, 21,  7, 12,  5,  1,  9,  6, 16, 13,\n",
    "       11, 14, 15,  3,  0],['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
    "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
    "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
    "       'Romanian', 'Russian', 'English', 'Arabic']))\n",
    "\n",
    "for k, v in sorted(d.items()): \n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3846f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9998787878787879\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       745\n",
      "           1       1.00      1.00      1.00       764\n",
      "           2       1.00      1.00      1.00       764\n",
      "           3       1.00      1.00      1.00       734\n",
      "           4       1.00      1.00      1.00       738\n",
      "           5       1.00      1.00      1.00       746\n",
      "           6       1.00      1.00      1.00       741\n",
      "           7       1.00      1.00      1.00       712\n",
      "           8       1.00      1.00      1.00       764\n",
      "           9       1.00      1.00      1.00       763\n",
      "          10       1.00      1.00      1.00       742\n",
      "          11       1.00      1.00      1.00       741\n",
      "          12       1.00      1.00      1.00       764\n",
      "          13       1.00      1.00      1.00       736\n",
      "          14       1.00      1.00      1.00       769\n",
      "          15       1.00      1.00      1.00       753\n",
      "          16       1.00      1.00      1.00       761\n",
      "          17       1.00      1.00      1.00       759\n",
      "          18       1.00      1.00      1.00       759\n",
      "          19       1.00      1.00      1.00       749\n",
      "          20       1.00      1.00      1.00       739\n",
      "          21       1.00      1.00      1.00       757\n",
      "\n",
      "    accuracy                           1.00     16500\n",
      "   macro avg       1.00      1.00      1.00     16500\n",
      "weighted avg       1.00      1.00      1.00     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_lr_train_pred = cv_lr.predict(X_cv_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_cv_train, y_cv_lr_train_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_cv_train, y_cv_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9feaf4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9501818181818181\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       255\n",
      "           1       0.79      0.61      0.69       236\n",
      "           2       1.00      0.97      0.99       236\n",
      "           3       0.89      0.98      0.93       266\n",
      "           4       0.97      0.96      0.97       262\n",
      "           5       0.99      0.98      0.99       254\n",
      "           6       1.00      0.98      0.99       259\n",
      "           7       1.00      0.94      0.97       288\n",
      "           8       0.55      0.91      0.68       236\n",
      "           9       1.00      0.94      0.97       237\n",
      "          10       0.97      0.94      0.96       258\n",
      "          11       1.00      0.99      0.99       259\n",
      "          12       0.98      1.00      0.99       236\n",
      "          13       1.00      0.94      0.97       264\n",
      "          14       1.00      0.97      0.98       231\n",
      "          15       1.00      0.93      0.96       247\n",
      "          16       0.99      0.97      0.98       239\n",
      "          17       1.00      0.98      0.99       241\n",
      "          18       1.00      0.98      0.99       241\n",
      "          19       1.00      0.96      0.98       251\n",
      "          20       1.00      0.97      0.99       261\n",
      "          21       1.00      0.98      0.99       243\n",
      "\n",
      "    accuracy                           0.95      5500\n",
      "   macro avg       0.96      0.95      0.95      5500\n",
      "weighted avg       0.96      0.95      0.95      5500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_lr_test_pred = cv_lr.predict(X_cv_test)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_cv_test, y_cv_lr_test_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_cv_test, y_cv_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9727675",
   "metadata": {},
   "source": [
    "Although there is 95% accuracy on the test data, the model is overfitting to the training data. F1 score for Chinese and Japanese are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39a21a",
   "metadata": {},
   "source": [
    "### Model 2: CountVectorizer + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4adc4",
   "metadata": {},
   "source": [
    "Now I will create a CountVectorizer with Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb7b2d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of CountVectorizer\n",
    "cv_nb = MultinomialNB()\n",
    "cv_nb.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0454eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9842424242424243\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       745\n",
      "           1       1.00      0.97      0.99       764\n",
      "           2       1.00      0.99      1.00       764\n",
      "           3       0.77      1.00      0.87       734\n",
      "           4       1.00      0.98      0.99       738\n",
      "           5       0.97      1.00      0.98       746\n",
      "           6       1.00      0.98      0.99       741\n",
      "           7       1.00      0.99      0.99       712\n",
      "           8       1.00      0.98      0.99       764\n",
      "           9       1.00      1.00      1.00       763\n",
      "          10       1.00      0.94      0.97       742\n",
      "          11       1.00      1.00      1.00       741\n",
      "          12       1.00      0.96      0.98       764\n",
      "          13       1.00      0.97      0.98       736\n",
      "          14       1.00      0.99      0.99       769\n",
      "          15       1.00      0.99      0.99       753\n",
      "          16       1.00      0.98      0.99       761\n",
      "          17       1.00      1.00      1.00       759\n",
      "          18       1.00      0.99      1.00       759\n",
      "          19       1.00      0.99      0.99       749\n",
      "          20       1.00      0.99      0.99       739\n",
      "          21       1.00      0.99      0.99       757\n",
      "\n",
      "    accuracy                           0.98     16500\n",
      "   macro avg       0.99      0.98      0.98     16500\n",
      "weighted avg       0.99      0.98      0.99     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_nb_train_pred = cv_nb.predict(X_cv_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_cv_train, y_cv_nb_train_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_cv_train, y_cv_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58640f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9543636363636364\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       255\n",
      "           1       0.92      0.56      0.69       236\n",
      "           2       0.97      0.98      0.98       236\n",
      "           3       0.69      1.00      0.81       266\n",
      "           4       0.98      0.97      0.98       262\n",
      "           5       0.95      0.98      0.97       254\n",
      "           6       1.00      0.99      0.99       259\n",
      "           7       1.00      0.95      0.98       288\n",
      "           8       0.74      0.81      0.77       236\n",
      "           9       1.00      0.98      0.99       237\n",
      "          10       0.98      0.91      0.94       258\n",
      "          11       1.00      1.00      1.00       259\n",
      "          12       0.99      0.97      0.98       236\n",
      "          13       1.00      0.95      0.98       264\n",
      "          14       0.99      1.00      0.99       231\n",
      "          15       0.99      0.99      0.99       247\n",
      "          16       0.96      0.97      0.97       239\n",
      "          17       0.99      1.00      1.00       241\n",
      "          18       1.00      0.99      0.99       241\n",
      "          19       1.00      0.99      1.00       251\n",
      "          20       1.00      0.99      0.99       261\n",
      "          21       1.00      0.99      0.99       243\n",
      "\n",
      "    accuracy                           0.95      5500\n",
      "   macro avg       0.96      0.95      0.95      5500\n",
      "weighted avg       0.96      0.95      0.95      5500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_nb_test_pred = cv_nb.predict(X_cv_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_cv_test, y_cv_nb_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_cv_test, y_cv_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3ce2d",
   "metadata": {},
   "source": [
    "Although there is 95% accuracy on the test data, the model is overfitting to the training data. F1 score for Chinese and Japanese are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c406be",
   "metadata": {},
   "source": [
    "### Model 3: Tf-idf + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b98e22",
   "metadata": {},
   "source": [
    "Now I will build the third model, Tf-idf with Logistic Regression. First I need to transform the text using the Tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdeabd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Tf-idf vectorizer and set X_tf equal to the transformed data\n",
    "tf = TfidfVectorizer()\n",
    "X_tf = tf.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "990c99c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d26701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1880fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of Tf-idf\n",
    "tf_lr = LogisticRegression(max_iter=1000)\n",
    "tf_lr.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "356f6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9866477272727273\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       783\n",
      "           1       0.96      0.99      0.97       793\n",
      "           2       1.00      0.98      0.99       791\n",
      "           3       0.87      0.99      0.93       808\n",
      "           4       1.00      0.99      0.99       814\n",
      "           5       0.98      0.99      0.99       801\n",
      "           6       1.00      0.98      0.99       789\n",
      "           7       1.00      0.98      0.99       796\n",
      "           8       0.95      0.99      0.97       812\n",
      "           9       1.00      1.00      1.00       784\n",
      "          10       0.99      0.98      0.98       806\n",
      "          11       1.00      0.99      0.99       797\n",
      "          12       1.00      0.98      0.99       813\n",
      "          13       1.00      0.96      0.98       789\n",
      "          14       1.00      0.99      0.99       790\n",
      "          15       1.00      1.00      1.00       797\n",
      "          16       1.00      0.98      0.99       824\n",
      "          17       1.00      1.00      1.00       797\n",
      "          18       1.00      0.99      0.99       788\n",
      "          19       1.00      0.99      1.00       807\n",
      "          20       1.00      0.99      1.00       805\n",
      "          21       1.00      0.98      0.99       816\n",
      "\n",
      "    accuracy                           0.99     17600\n",
      "   macro avg       0.99      0.99      0.99     17600\n",
      "weighted avg       0.99      0.99      0.99     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_lr_train_pred = tf_lr.predict(X_tf_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_train, y_tf_lr_train_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_train, y_tf_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25e6390e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9561363636363637\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       217\n",
      "           1       0.83      0.71      0.76       207\n",
      "           2       1.00      0.98      0.99       209\n",
      "           3       0.81      0.99      0.89       192\n",
      "           4       0.99      0.96      0.97       186\n",
      "           5       0.98      0.98      0.98       199\n",
      "           6       1.00      0.96      0.98       211\n",
      "           7       1.00      0.97      0.99       204\n",
      "           8       0.62      0.94      0.75       188\n",
      "           9       1.00      0.96      0.98       216\n",
      "          10       0.98      0.94      0.96       194\n",
      "          11       1.00      0.99      0.99       203\n",
      "          12       1.00      0.96      0.98       187\n",
      "          13       1.00      0.91      0.96       211\n",
      "          14       1.00      0.99      0.99       210\n",
      "          15       1.00      0.96      0.98       203\n",
      "          16       0.99      0.98      0.99       176\n",
      "          17       1.00      1.00      1.00       203\n",
      "          18       1.00      1.00      1.00       212\n",
      "          19       1.00      0.98      0.99       193\n",
      "          20       1.00      0.95      0.98       195\n",
      "          21       1.00      0.96      0.98       184\n",
      "\n",
      "    accuracy                           0.96      4400\n",
      "   macro avg       0.96      0.96      0.96      4400\n",
      "weighted avg       0.96      0.96      0.96      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_lr_test_pred = tf_lr.predict(X_tf_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_test, y_tf_lr_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_test, y_tf_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b417fa",
   "metadata": {},
   "source": [
    "Although there is 95% accuracy on the test data, the model is overfitting to the training data F1 score for Chinese and Japanese are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be581f",
   "metadata": {},
   "source": [
    "### Model 4: Tf-idf + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d876b4",
   "metadata": {},
   "source": [
    "I will now create the fourth model: Tf-idf with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "901be54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of Tf-idf\n",
    "tf_nb = MultinomialNB()\n",
    "tf_nb.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "081914a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.98375\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       783\n",
      "           1       1.00      0.95      0.98       793\n",
      "           2       1.00      0.98      0.99       791\n",
      "           3       0.77      1.00      0.87       808\n",
      "           4       1.00      0.98      0.99       814\n",
      "           5       0.96      0.99      0.98       801\n",
      "           6       1.00      0.98      0.99       789\n",
      "           7       1.00      0.98      0.99       796\n",
      "           8       1.00      0.98      0.99       812\n",
      "           9       1.00      1.00      1.00       784\n",
      "          10       1.00      0.94      0.97       806\n",
      "          11       1.00      1.00      1.00       797\n",
      "          12       1.00      0.97      0.98       813\n",
      "          13       1.00      0.97      0.98       789\n",
      "          14       1.00      0.98      0.99       790\n",
      "          15       0.99      0.99      0.99       797\n",
      "          16       0.99      0.98      0.99       824\n",
      "          17       1.00      1.00      1.00       797\n",
      "          18       1.00      0.99      0.99       788\n",
      "          19       1.00      0.99      0.99       807\n",
      "          20       1.00      0.99      1.00       805\n",
      "          21       1.00      0.99      0.99       816\n",
      "\n",
      "    accuracy                           0.98     17600\n",
      "   macro avg       0.99      0.98      0.98     17600\n",
      "weighted avg       0.99      0.98      0.98     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_nb_train_pred = tf_nb.predict(X_tf_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_train, y_tf_nb_train_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_tf_train, y_tf_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "528b0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9545454545454546\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       217\n",
      "           1       0.97      0.56      0.71       207\n",
      "           2       0.99      0.98      0.98       209\n",
      "           3       0.63      0.99      0.77       192\n",
      "           4       0.99      0.95      0.97       186\n",
      "           5       0.95      0.99      0.97       199\n",
      "           6       1.00      0.96      0.98       211\n",
      "           7       0.98      0.99      0.98       204\n",
      "           8       0.76      0.87      0.81       188\n",
      "           9       1.00      0.98      0.99       216\n",
      "          10       0.99      0.92      0.95       194\n",
      "          11       1.00      1.00      1.00       203\n",
      "          12       0.98      0.96      0.97       187\n",
      "          13       1.00      0.94      0.97       211\n",
      "          14       1.00      1.00      1.00       210\n",
      "          15       1.00      0.99      0.99       203\n",
      "          16       0.98      0.99      0.99       176\n",
      "          17       0.99      1.00      1.00       203\n",
      "          18       1.00      1.00      1.00       212\n",
      "          19       1.00      0.98      0.99       193\n",
      "          20       0.98      0.98      0.98       195\n",
      "          21       1.00      0.98      0.99       184\n",
      "\n",
      "    accuracy                           0.95      4400\n",
      "   macro avg       0.96      0.95      0.95      4400\n",
      "weighted avg       0.96      0.95      0.95      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_nb_test_pred = tf_nb.predict(X_tf_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_test, y_tf_nb_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_test, y_tf_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7645c",
   "metadata": {},
   "source": [
    "Although there is 95% accuracy on the test data, the model is overfitting to the training data. F1 score for Chinese, Japanese, and Hindi are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ecad91",
   "metadata": {},
   "source": [
    "### Hyperparameter Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87449e15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data Accuracy</th>\n",
       "      <th>Test Data Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.950182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.984242</td>\n",
       "      <td>0.954364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.986648</td>\n",
       "      <td>0.956136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.983750</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Training Data Accuracy  \\\n",
       "Count Vectorizer Logistic Regression                0.999879   \n",
       "                 Naive Bayes                        0.984242   \n",
       "Tf-idf           Logistic Regression                0.986648   \n",
       "                 Naive Bayes                        0.983750   \n",
       "\n",
       "                                      Test Data Accuracy  \n",
       "Count Vectorizer Logistic Regression            0.950182  \n",
       "                 Naive Bayes                    0.954364  \n",
       "Tf-idf           Logistic Regression            0.956136  \n",
       "                 Naive Bayes                    0.954545  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies of the four models for training and test data.\n",
    "data = [[accuracy_score(y_cv_train, y_cv_lr_train_pred), accuracy_score(y_cv_test, y_cv_lr_test_pred)], \n",
    "        [accuracy_score(y_cv_train, y_cv_nb_train_pred), accuracy_score(y_cv_test, y_cv_nb_test_pred)],\n",
    "        [accuracy_score(y_tf_train, y_tf_lr_train_pred), accuracy_score(y_tf_test, y_tf_lr_test_pred)], \n",
    "        [accuracy_score(y_tf_train, y_tf_nb_train_pred), accuracy_score(y_tf_test, y_tf_nb_test_pred)]\n",
    "       ]\n",
    "\n",
    "accuracy_df = pd.DataFrame(data, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Logistic Regression', 'Naive Bayes', 'Logistic Regression','Naive Bayes']],\n",
    "                          columns = ['Training Data Accuracy','Test Data Accuracy'])\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6b29f",
   "metadata": {},
   "source": [
    "From the data we can see that in terms of accuracy, the best model is Tf-idf with Logistic Regression (it has the highest accuracy on the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb69631",
   "metadata": {},
   "source": [
    "### Cross Validation Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c464dc",
   "metadata": {},
   "source": [
    "To more accurately assess model performance, I will examine the mean cross validation score of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4e53246",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr_cval = cross_val_score(cv_lr, X_cv, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3336fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_nb_cval = cross_val_score(cv_nb, X_cv, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9af4ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lr_cval = cross_val_score(tf_lr, X_tf, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb4ae024",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_nb_cval = cross_val_score(tf_nb, X_tf, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b98d4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cross Validation Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.947773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.955409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Cross Validation Score\n",
       "Count Vectorizer Logistic Regression                0.947773\n",
       "                 Naive Bayes                        0.955409\n",
       "Tf-idf           Logistic Regression                0.955000\n",
       "                 Naive Bayes                        0.954545"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies of the four models after cross validation.\n",
    "data_cval = [cv_lr_cval, cv_nb_cval, tf_lr_cval, tf_nb_cval]\n",
    "cross_val_scores = pd.DataFrame(data_cval, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Logistic Regression', 'Naive Bayes', 'Logistic Regression','Naive Bayes']],\n",
    "                          columns = ['Cross Validation Score'])\n",
    "cross_val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7b27d",
   "metadata": {},
   "source": [
    "Count Vectorizer with Naive Bayes is the best model. So, now I will do a grid search to find the optimal parameters in order to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7136adf",
   "metadata": {},
   "source": [
    "### Reduce Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d5835f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MultinomialNB(),\n",
       "             param_grid={'alpha': [0.01, 0.1, 1, 10, 100, 1000]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a grid search to find the optimal alpha value.\n",
    "lang_gs = GridSearchCV(cv_nb, param_grid={'alpha':[0.01, 0.1, 1, 10, 100, 1000]})\n",
    "lang_gs.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ad01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9916969696969697\n",
      "{'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "gs_nb_train_pred = lang_gs.predict(X_cv_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_cv_train, gs_nb_train_pred))\n",
    "print(lang_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "042628df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9603636363636363\n",
      "{'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "gs_nb_test_pred = lang_gs.predict(X_cv_test)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_cv_test, gs_nb_test_pred))\n",
    "print(lang_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc6668",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775194c",
   "metadata": {},
   "source": [
    "In the next section, I will use the models I have created to predict the langauge of a given paragraph of text. The text has been gathered from different websites in various languages. In total, there are 22 paragraphs, one for each langauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a73e70f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8a1dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for each langauge\n",
    "# Estonian. Source: https://uueduudised.ee/uudis/eesti/ekre-ettepanek-homofilmifestivali-raha-ukraina-kultuuriseltsile-anda-ei-leidnud-rakveres-toetust/\n",
    "a = \"Rakvere linnavolikokku kuuluvates Eesti Konservatiivse Rahvaerakonna saadikutes tekitas küsimusi homofilmifestivali Festheart rahastamine ajal, mil Ukrainas käib sõda ja selle asemel võiks linna eelarves homopropagandale eraldatava kultuurirahaga toetada pigem Ukraina kultuuriseltsi.\"\n",
    "\n",
    "# Swedish. Source: https://www.svt.se/sport/ishockey/mallost-efter-forsta-perioden-i-odesmatchen\n",
    "b = \"Grabbarna känns verkligen laddade för uppgiften, men det är 40 långa minuter kvar, sa Djurgårdens Sebastian Strandberg i C Mores sändning efter de första 20 minuterna. Halvvägs in i ångestmatchen tog Timrå ledningen med 1-0 genom Robin Hanzl, som styrde in matchens första mål, innan Ty Rattie, 56 sekunder senare, utökade till 2-0. Hanzl blev också tvåmålsskytt när Djurgården gav bort pucken i egen zon och släppte in ett tredje mål.\"\n",
    "\n",
    "# Thai. Source: https://nlovecooking.com/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3%E0%B9%84%E0%B8%97%E0%B8%A2-2/\n",
    "c = \"คุณค่าของอาหารไทยด้านวัฒนธรรม การถ่ายทอดความรู้ด้านการทำอาหารใน อาหารไทย นั้น แสดงถึงภูมิปัญญาของคนไทย และ วัฒนธรรมด้านอาหารของคนไทย บ่งบอกถึงความเจริญของชนชาตินั้นๆ อาหารไทย มีเอกลักษณ์ที่แตกต่างจากอาหารของชนชาติอื่นๆ สามารถปรับปรุงรสชาติให้เข้ากับคนุกชาติได้ จึงแสดงถึงคุณค่าของอาหารไทย ที่ทำให้คนทั่วโลกยอมรับ\"\n",
    "\n",
    "# Tamil. Source: https://artsandculture.google.com/entity/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AE%B0%E0%AF%8D-%E0%AE%B5%E0%AE%B0%E0%AE%B2%E0%AE%BE%E0%AE%B1%E0%AF%81/g11cls_rl0p?hl=ta\n",
    "d = \"தமிழர் மத்திய ஆசியா, வட இந்தியா நிலப்பரப்புகளில் இருந்து காலப்போக்கில் தென் இந்தியா வந்தனர் என்பது மற்றைய கருதுகோள். எப்படி இருப்பினும் தமிழர் இனம் தொன்மையான மக்கள் இனங்களில் ஒன்று. தமிழர்களின் தோற்றம் மற்ற திராவிடர்களைப் போலவே இன்னும் தெளிவாக அறியப்படவில்லை.\"\n",
    "\n",
    "# Dutch. Source: https://www.stuivengalederwaren.nl/leukste-hollandse-tassen/\n",
    "e = \"Berba staat vooral bekend om de zachte leren tassen en bijpassende portemonnees. En met de vele vakjes en een lange schouderbanden sluiten de tassen én portemonnees perfect aan bij de wensen van de Hollandse vrouw (en man!). Zo heb je met Berba dé ideale combinatie van schoonheid en functionaliteit.\"\n",
    "\n",
    "# Japanese. Source: https://twitter.com/twitterjp/status/923671036758958080\n",
    "f = \"いつも、そして何年もの間、Twitterをご利用いただきありがとうございます。おかげさまで日本での月間利用者数が4500万を超えました。安心してサービスをご利用いただけますように、一層の努力を行います。引き続きのご指導、ご支援のほど、よろしくお願い申し上げます\"\n",
    "\n",
    "# Turkish. Source: https://www.haberturk.com/seren-serengil-e-annesi-nevin-serengil-den-isyan-3396288-magazin\n",
    "g = \"Kimi varlıkla imtihan edilir, kimi yoklukla... Kimi hastalıkla imtihan edilir, kimi sağlıkla... Ama evlatla imtihan edilmek imtihanların en zorudur. Çünkü canını yakan yine kendi canındır. Bin parçaya da bölünürsün ama yine de nefret edemezsin. Rabbim hiç kimseyi evlatlarıyla imtihan etmesin.\"\n",
    "\n",
    "# Latin. Source: https://www.lipsum.com/\n",
    "h = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "# Urdu. Source: https://www.urdunews.com/node/658036\n",
    "i = \"انہوں نے کہا کہ پاکستان میں رجسٹر اور غیر رجسٹرڈ افغان مہاجرین کی تعداد 40 لاکھ کے لگ بھگ ہے۔آرمی چیف نے مشرقی سرحد کی صورتحال پر کہا کہ لائن آف کنٹرول پر حالات بہتر ہیں اور وہاں بسنے والے شہریوں کی زندگی میں امن آیا ہے۔ان کا کہنا تھا کہ انڈین سپر سونک میزائل کے پاکستان میں گرنے کا واقعہ انتہائی تشویشناک ہے۔ عالمی برادری اس کا نوٹس لے گی کیونکہ اس سے یہاں عام شہریوں کا جانی نقصان بھی ہو سکتا تھا جبکہ اس میزائل کے راستے میں آنے والا کوئی مسافر طیارہ بھی نشانہ بن سکتا تھا۔\"\n",
    "\n",
    "# Indonesian. Source: https://news.detik.com/berita/d-6013602/ingat-13-lokasi-di-jakarta-ditutup-jelang-sahur-pukul-0100-0500-wib\n",
    "j = \"Filterisasi mengantisipasi sahur on the road atau SOTR dilakukan Polda Metro Jaya di wilayah DKI Jakarta selama bulan Ramadan. Perlu diingat, total ada 13 lokasi yang diberlakukan filterisasi pada jam-jam menjelang sahur.\"\n",
    "\n",
    "# Protugese. Source: https://www.dn.pt/internacional/ucrania-acusa-tropas-russas-de-abrirem-fogo-contra-manifestantes-pacificos-14737367.html\n",
    "k = \"'Hoje em Energodar, os moradores da cidade reuniram-se de novo manifestando-se em apoio da Ucrânia e cantando o hino nacional', postou na rede social Facebook a responsável pelos Direitos Humanos no Parlamento ucraniano, Lyoudmyla Denisova.\"\n",
    "\n",
    "# French. Source: https://www.francetvinfo.fr/elections/presidentielle/presidentielle-2022-ces-12-millions-de-francais-encore-indecis_5059294.html\n",
    "l = \"Le 10 avril se tiendra le premier tour de l'élection présidentielle. Vendredi 1er avril, 37 % des électeurs ne savent toujours pas pour qui ils vont voter. Ces indécis sont des personnes qui sont certaines d'aller voter, mais qui peuvent changer d'avis. Un citoyen hésite ainsi entre Yannick Jadot (EELV) et Emmanuel Macron (LREM). Une autre dit se laisser encore quelques jours pour consulter les programmes. Près de 6 sur 10 électeurs de Yannick Jadot, Anne Hidalgo (PS) et Fabien Roussel (PCF) sont indécis.\"\n",
    "\n",
    "# Chinese. Source: https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\n",
    "m = \"1949年，以毛泽东主席为领袖的中国共产党领导中国人民解放军在内战中取得优势，实际控制中国大陆，同年10月1日宣布建立中华人民共和国以及中央人民政府，与迁至台湾地区的中华民国政府形成至今的台海现状格局。中华人民共和国成立初期遵循和平共处五项原则的外交政策，1971年在联合国取得了原属于中华民国的中国代表权及其联合国安理会常任理事国席位，并陆续加入部分联合国其他专门机构。而后广泛参与例如国际奥委会、亚太经合组织、二十国集团、世界贸易组织等重要国际组织，并成为上海合作组织、金砖国家、一带一路、亚洲基础设施投资银行、区域全面经济伙伴关系协定等国际合作组织项目的发起国和创始国。据皮尤研究中心的调查，随着国际影响力的增强，中华人民共和国已被许多国家、组织视为世界经济的重要支柱与潜在超级大国之一[41][42][43]。\"\n",
    "\n",
    "# Korean. Source: https://news.kbs.co.kr/news/view.do?ncd=5430540\n",
    "n = \"호남 출신인 한 전 총리는 경제 관료 출신으로, 김대중 정부에서 청와대 경제수석, 노무현 정부에서 국무총리를 지냈고, 이명박 정부에서 주미 대사를, 박근혜 정부에서는 무역협회장을 역임했습니다. 가장 중요한 건 경제라던 윤 당선인은 한 전 총리에 대해 '통합형 총리'에 맞고, 외교와 통상, 경제 전문가로서의 경륜을 높이 사고 있다고 말한 것으로 전해졌습니다. 2007년 총리 후보자로 국회 인사청문회를 통과했던 만큼, 민주당이 다수인 국회에서의 임명 동의 등 여러 측면을 고려한 인선이란 분석도 나옵니다.\"\n",
    "\n",
    "# Hindi. Source: https://www.bbc.com/hindi/india-60964637\n",
    "o = \"भारत दौरे पर आए नेपाल के प्रधानमंत्री शेर बहादुर देउबा की शनिवार को प्रधानमंत्री नरेंद्र मोदी समेत कई महत्वपूर्ण नेताओं से मुलाकात हुई. साथ ही भारत और नेपाल ने शनिवार को सीमा पार रेलवे नेटवर्क समेत कई विकास परियोजनाओं का उद्घाटन किया. इस मौके पर नेपाल के प्रधानमंत्री शेर बहादुर देउबा ने कहा कि दोनों देशों के बीच चल रहे सीमा विवाद को सुलझाने के लिए कोई साझा व्यवस्था बने.\"\n",
    "\n",
    "# Spanish. Source: https://cnnespanol.cnn.com/2022/04/02/analisis-putin-esta-cometiendo-los-mismos-errores-que-condenaron-a-hitler-trax/\n",
    "p = \"Pero los tanques rusos se han visto obstaculizados por otra razón sorprendente: la falta de combustible. La falta de combustible es parte de un problema mayor. El ejército ruso, del que alguna vez se alardeó se ha estancado en Ucrania no solo por la feroz resistencia, sino por algo más prosaico: la logística.\"\n",
    "\n",
    "# Pushto. Source: https://www.bbc.com/pashto/world-60909321\n",
    "q = \"ملګري ملتونه وايي نژدې دوه ميلیونه اوکرايني ماشومان اوس د روسيې له بمبارۍ ګاونډیو هېوادونو ته تښتېدلي دي. يونيسېف او د بشري مرستو نورو ټولنو خبرداری ورکړی، دا ماشومان یې له خپلو ميندو او نورو ښځينه اوکراينيو کډوالو سره د قاچاق او ناوړه ګټې اخيستو لوړې کچې خطر سره مخامخ دي.\"\n",
    "\n",
    "# Persian. Source: https://www.bbc.com/persian/afghanistan-60966238\n",
    "r = \"گزارش‌های قبلا به نقل از طالبان طالبان منتشر شده بود که این گروه برای آزادی مارک فرریکس خواستار رهایی یک افغان به نام بشیر نورزی شده بوده است که در حال گذراندن محکومیت حبس ابد به جرم قاچاق مواد مخدر در ایالات متحده است.\"\n",
    "\n",
    "# Romanian. Source: https://www.digi24.ro/stiri/externe/sua-trimite-ucrainei-echipament-de-protectie-in-caz-de-atacuri-chimice-zelenski-rusii-planuiesc-atacuri-puternice-in-donbas-si-harkov-1891921\n",
    "s = \"Președintele ucrainean Volodimir Zelenski spune că retragerea trupelor rusești din nordul țării este „înceată dar vizibilă”. Acesta avertizează însă ucrainenii că vor urma „lupte grele” în estul țării, în zonele Donbas și Harkov. Peste 3.000 de oameni au reușit să părăsească orașul-port Mariupol, mai spune președintele Ucrainei. Între timp, SUA ajută țara pentru posibile atacuri chimice, trimițând echipament personal de protecție. De asemenea, Pentagonul va oferi Ucrainei un ajutor militar suplimentar de până la 300 de milioane de dolari.\"\n",
    "\n",
    "# Russian. Source: https://ria.ru/20220402/protesty-1781464774.html\n",
    "t = \"Таким образом, расходы британцев на энергию вырастут в среднем на 700 фунтов в год и составят около двух тысяч. Из-за этого годовая инфляция в феврале достигла в Британии рекордного за 30 лет уровня — 6,2 процента.\"\n",
    "\n",
    "# English. Source: https://www.wsj.com/articles/tesla-deliveries-rose-in-quarter-elon-musk-calls-exceptionally-difficult-11648917258?mod=hp_lead_pos2\n",
    "u = \"Tesla Inc. vehicle deliveries rose in the first quarter, but missed Wall Street expectations as the company struggled with global supply-chain disruptions and a brief Covid-19 shutdown at its Shanghai factory. This was an *exceptionally* difficult quarter due to supply chain interruptions & China zero Covid policy,” Tesla Chief Executive Elon Musk tweeted Saturday morning. Tesla employees and key suppliers 'saved the day,' he added.\"\n",
    "\n",
    "# Arabic.\n",
    "v = \"وقال المتحدث باسم الوزارة أحمد الصحاف، إن 'الوزير فؤاد حسين استقبل اليوم سفراء مجموعة G7 المعتمدين لدى العراق، واستعرض تفاصيل وأبعاد زيارته المرتقبة إلى موسكو ووارسو ضمن مجموعة الاتصال العربية على المستوى وزارء في جامعة الدول العربية لمتابعة وإجراء المشاورات والاتصالات اللازمة مع الأطراف المعنية بالأزمة الروسية-الأوكرانية بهدف المساهمة في إيجاد الحلول الدبلوماسية للازمة وإنهاء الحرب القائم'.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cfa0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0f08c",
   "metadata": {},
   "source": [
    "### Count Vectorizer Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7190eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Logistic Regression Model\n",
    "def predict_cv_lr(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41d1bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5209a958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Japanese\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese       Japanese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0005b",
   "metadata": {},
   "source": [
    "The only prediction that is inaccurate is Chinese, since the function predicted Japanese instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2fbd2",
   "metadata": {},
   "source": [
    "### Count Vectorizer Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae871a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Naive Bayes Model\n",
    "def predict_cv_nb(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44022085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86fc6f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Romanian\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e4762",
   "metadata": {},
   "source": [
    "This time, Japanese is predicted as Romanian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1963f",
   "metadata": {},
   "source": [
    "### Tf-idf Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcbffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_lr(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86923c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "146a2957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Japanese\n",
       "Turkish       Japanese\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2144c",
   "metadata": {},
   "source": [
    "This model predicts all but Turkish correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb771280",
   "metadata": {},
   "source": [
    "### Tf-idf Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0acf901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_nb(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e4d401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f64c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese       Spanish\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa6785",
   "metadata": {},
   "source": [
    "All but Japanese are predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dd8aa",
   "metadata": {},
   "source": [
    "Based on the precision and recall scores I have from the trained models, along with the fact that Japanese and Chinese texts are difficult to predict, makes me think that Chinese and Japanese are not being properly vectorized since there are no spaces between words in these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfd7ba",
   "metadata": {},
   "source": [
    "### Model that Tokenizes Chinese and Japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358f2ff",
   "metadata": {},
   "source": [
    "Now, I will build a new model that incorporates tokenized Chinese and Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e4e7c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Turkish',\n",
       "       'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French', 'Korean',\n",
       "       'Hindi', 'Spanish', 'Pushto', 'Persian', 'Romanian', 'Russian',\n",
       "       'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the three dataframes\n",
    "cn = df[df['language'] == 'Chinese']\n",
    "jp = df[df['language'] == 'Japanese']\n",
    "other_langs = df[df['language'] != 'Chinese']\n",
    "other_langs = other_langs[other_langs['language'] != 'Japanese']\n",
    "other_langs['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28c8ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will tokenize Chinese\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "# Define a function that will tokenize Japanese\n",
    "def tokenize_jp(text):\n",
    "    text = nagisa.tagging(text)\n",
    "    return text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10e06be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/bn/55fdvsy52pl19l11rfd0vm340000gn/T/jieba.cache\n",
      "Dumping model to file cache /var/folders/bn/55fdvsy52pl19l11rfd0vm340000gn/T/jieba.cache\n",
      "Loading model cost 0.579 seconds.\n",
      "Loading model cost 0.579 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1000x26575 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86073 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document term matrix for Chinese\n",
    "vectorizer_cn = CountVectorizer(tokenizer=tokenize_zh)\n",
    "cn_dtm = vectorizer_cn.fit_transform(cn['Text'])\n",
    "cn_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "644e5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x15244 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72711 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document term matrix for Japanese\n",
    "vectorizer_jp = CountVectorizer(tokenizer=tokenize_jp)\n",
    "jp_dtm = vectorizer_jp.fit_transform(jp['Text'])\n",
    "jp_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c8831ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x245462 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 876596 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document term matrix for the other languages\n",
    "vectorizer_other_langs = CountVectorizer()\n",
    "other_langs_dtm = vectorizer_other_langs.fit_transform(other_langs['Text'])\n",
    "other_langs_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fd2d5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22000x287281 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1389204 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the three document term matrices using FeatureUnion.\n",
    "merged_dtm = FeatureUnion([('CountVectorizer', vectorizer_cn),('CountVect', vectorizer_jp),('Count',vectorizer_other_langs)])\n",
    "dtm = merged_dtm.transform(df['Text'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bacbc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_langs_train, X_langs_test, y_langs_train, y_langs_test = train_test_split(dtm, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9818a0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs_nb = MultinomialNB(alpha=0.1)\n",
    "langs_nb.fit(X_langs_train, y_langs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2073a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9875151515151516\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       750\n",
      "           1       0.99      0.99      0.99       750\n",
      "           2       1.00      1.00      1.00       750\n",
      "           3       0.81      1.00      0.90       750\n",
      "           4       1.00      0.98      0.99       750\n",
      "           5       0.99      0.99      0.99       750\n",
      "           6       1.00      0.98      0.99       750\n",
      "           7       1.00      0.98      0.99       750\n",
      "           8       1.00      0.99      0.99       750\n",
      "           9       1.00      0.99      0.99       750\n",
      "          10       0.99      0.96      0.98       750\n",
      "          11       1.00      1.00      1.00       750\n",
      "          12       1.00      0.98      0.99       750\n",
      "          13       1.00      0.96      0.98       750\n",
      "          14       1.00      0.99      1.00       750\n",
      "          15       0.99      0.99      0.99       750\n",
      "          16       1.00      0.99      0.99       750\n",
      "          17       1.00      1.00      1.00       750\n",
      "          18       1.00      0.99      0.99       750\n",
      "          19       1.00      0.99      0.99       750\n",
      "          20       1.00      0.99      1.00       750\n",
      "          21       1.00      0.99      0.99       750\n",
      "\n",
      "    accuracy                           0.99     16500\n",
      "   macro avg       0.99      0.99      0.99     16500\n",
      "weighted avg       0.99      0.99      0.99     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_langs_train_pred = langs_nb.predict(X_langs_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_langs_train, y_langs_train_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_langs_train, y_langs_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "890d04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9803636363636363\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       250\n",
      "           1       0.99      0.99      0.99       250\n",
      "           2       0.98      0.99      0.99       250\n",
      "           3       0.78      0.99      0.87       250\n",
      "           4       0.99      0.95      0.97       250\n",
      "           5       0.97      0.99      0.98       250\n",
      "           6       1.00      0.98      0.99       250\n",
      "           7       0.99      0.98      0.99       250\n",
      "           8       1.00      1.00      1.00       250\n",
      "           9       1.00      0.98      0.99       250\n",
      "          10       0.98      0.94      0.96       250\n",
      "          11       1.00      1.00      1.00       250\n",
      "          12       0.98      0.95      0.97       250\n",
      "          13       1.00      0.97      0.98       250\n",
      "          14       0.99      0.98      0.99       250\n",
      "          15       0.98      0.99      0.99       250\n",
      "          16       0.99      0.98      0.98       250\n",
      "          17       0.99      1.00      0.99       250\n",
      "          18       1.00      0.99      0.99       250\n",
      "          19       1.00      0.98      0.99       250\n",
      "          20       1.00      0.98      0.99       250\n",
      "          21       1.00      0.98      0.99       250\n",
      "\n",
      "    accuracy                           0.98      5500\n",
      "   macro avg       0.98      0.98      0.98      5500\n",
      "weighted avg       0.98      0.98      0.98      5500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_langs_test_pred = langs_nb.predict(X_langs_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_langs_test, y_langs_test_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_langs_test, y_langs_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f04c9",
   "metadata": {},
   "source": [
    "This model performs much better on Asian langauges than the previous best model. 98% accuracy on the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fedcba",
   "metadata": {},
   "source": [
    "## Predicting the First Sentence of a Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003c3ff",
   "metadata": {},
   "source": [
    "Now I will build a model to predict whether a sentence is the first in a paragraph or not.\n",
    "\n",
    "First, we need to create a new dataset which contains only one langauge. In this case, I will use Chinese since it is one of the only langauges in the datasets which still contains punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9be651f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>年月日，參與了「snh第三屆年度金曲大賞best 」。月日，出演由优酷视频，盟将威影视，嗨乐...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>在他们出发之前，罗伯特·菲茨罗伊送给了达尔文一卷查尔斯·赖尔所著《地质学原理》（在南美他得到...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>系列的第一款作品《薩爾達傳說》（ゼルダの伝説）在年月日於日本發行，之後在年內於美國和歐洲地區...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>历史上的柔远驿是为了给琉球贡使及随员提供食宿之所，同时它也成为中琉间商业和文化交流的枢纽。琉...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text language\n",
       "13   胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...  Chinese\n",
       "110  年月日，參與了「snh第三屆年度金曲大賞best 」。月日，出演由优酷视频，盟将威影视，嗨乐...  Chinese\n",
       "122  在他们出发之前，罗伯特·菲茨罗伊送给了达尔文一卷查尔斯·赖尔所著《地质学原理》（在南美他得到...  Chinese\n",
       "151  系列的第一款作品《薩爾達傳說》（ゼルダの伝説）在年月日於日本發行，之後在年內於美國和歐洲地區...  Chinese\n",
       "227  历史上的柔远驿是为了给琉球贡使及随员提供食宿之所，同时它也成为中琉间商业和文化交流的枢纽。琉...  Chinese"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn = df[df['language'] == 'Chinese']\n",
    "cn.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98faeb86",
   "metadata": {},
   "source": [
    "Before proceeding any further, I will do the train-test split on the data so that I am not splitting over paragraphs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61ef7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data.\n",
    "X_cn_train, X_cn_test, y_cn_train, y_cn_test = train_test_split(cn['Text'], cn['language'], test_size = 0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdbcce",
   "metadata": {},
   "source": [
    "Next, I will use spaCy to split the paragraphs up into individual sentneces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6fb8edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate spacy model\n",
    "nlp = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f3fc5",
   "metadata": {},
   "source": [
    "Now I will create a function which can create a new dataframe out of the original dataframe. The new dataframe will consist of sentences taken from the paragraphs, and each sentence will be labeled with a 1 or 0, representing being the first sentence in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01c23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the function that will label sentences as a first sentence or not.\n",
    "def first_sent(sentences, sent):\n",
    "    if sent == sentences[0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Now create the function which takes in a dataframe and creates a new dataframe.\n",
    "def new_df(df):\n",
    "    \n",
    "    # Create a list containing all of the spacy doc objects, one for each paragraph.\n",
    "    docs = []\n",
    "    for i in range(df.shape[0]):\n",
    "        doc = nlp(df.iloc[i])\n",
    "        docs.append(doc) \n",
    "\n",
    "    # Create a list containing all of the lists of each paragraph's sentences.\n",
    "    paragraphs = []\n",
    "    for doc in docs:\n",
    "        sentences = list(doc.sents)\n",
    "        paragraphs.append(sentences)\n",
    "\n",
    "    # Build a dictionary that will contain all of the sentences across all \n",
    "    # paragraphs and label whether each entry is the first sentence in the paragraph or not.      \n",
    "    sentences_dict = [{'Sentence':str(sent),'First':first_sent(sentences, sent)} for sentences in paragraphs for sent in sentences]\n",
    "\n",
    "    # Create a dataframe from the dictionary.\n",
    "    new_df = pd.DataFrame(sentences_dict)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7dccb",
   "metadata": {},
   "source": [
    "Now I will use the function to create new training and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83122381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>First</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>光绪三十二年年六月，顺天府在今西什库天财库旧址（今称后库）筹设顺天中学堂，并派藩祖荫作监督。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>光绪三十三年年是顺天中学堂正式创校之年。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>顺天四路学堂名学生入学肄业东路名、西路名、南路名、北路名，称为甲班今称第一级，于年十二月毕业。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>学制为四年，设国文、算术、历史、英文、社会学、国画等课程。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>宣统元年年正月，学堂添招乙班（英文班），学生三十一名（今称第二级，于年七月毕业）。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>宣统二年年正月，添招丙班（英文班），学生三十九名（今称第三级，于年正月毕业）。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>鞠婧禕（年月日－）</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中國大陸華語樂壇歌手、演员。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>出生於四川省遂寧市，是女子偶像組合snh成員，同時也曾是塞納河組合的成員。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>經紀公司為上海絲芭文化傳媒集团有限公司。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence  First\n",
       "0   光绪三十二年年六月，顺天府在今西什库天财库旧址（今称后库）筹设顺天中学堂，并派藩祖荫作监督。      1\n",
       "1                             光绪三十三年年是顺天中学堂正式创校之年。      0\n",
       "2  顺天四路学堂名学生入学肄业东路名、西路名、南路名、北路名，称为甲班今称第一级，于年十二月毕业。      0\n",
       "3                    学制为四年，设国文、算术、历史、英文、社会学、国画等课程。      0\n",
       "4        宣统元年年正月，学堂添招乙班（英文班），学生三十一名（今称第二级，于年七月毕业）。      0\n",
       "5          宣统二年年正月，添招丙班（英文班），学生三十九名（今称第三级，于年正月毕业）。      0\n",
       "6                                        鞠婧禕（年月日－）      1\n",
       "7                                   中國大陸華語樂壇歌手、演员。      0\n",
       "8            出生於四川省遂寧市，是女子偶像組合snh成員，同時也曾是塞納河組合的成員。      0\n",
       "9                             經紀公司為上海絲芭文化傳媒集团有限公司。      0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_train_new = new_df(X_cn_train)\n",
    "cn_train_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3589797f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>First</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>顺天中学堂选址于位于京师后库（现北京市西城区后库）的宛平高等小学堂。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>校园虽经多次翻新、重建，地址未曾改变。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>全校占地面积为亩，体育场占地亩，包括篮球场，网球场，排球场和足球场。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>校园内有树木余株，花卉余种。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>校园南院有植物园地，种有农作物，并建有一个古朴的井亭，颇有田园意境。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>如当时校歌所描述的：“半似乡村半似城，花木苍翠四时荣”。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>經典物理學通常用以闡述日常可觀察尺寸的系統現象，而現代物理學通常用以闡述極端或非常大尺寸、非...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>例如，化學元素可以被辨識的最小尺寸是原子物理學或核子物理學探索物質所操作的尺寸。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>而粒子物理學操作的尺寸則更為微小，它論述的是基本粒子或由基本粒子組成的粒子。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>由於使用大型粒子加速器來產生基本粒子需要非常巨大的能量，所以通常粒子物理學又稱為高能量物理學。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  First\n",
       "0                 顺天中学堂选址于位于京师后库（现北京市西城区后库）的宛平高等小学堂。      1\n",
       "1                                校园虽经多次翻新、重建，地址未曾改变。      0\n",
       "2                 全校占地面积为亩，体育场占地亩，包括篮球场，网球场，排球场和足球场。      0\n",
       "3                                     校园内有树木余株，花卉余种。      0\n",
       "4                 校园南院有植物园地，种有农作物，并建有一个古朴的井亭，颇有田园意境。      0\n",
       "5                       如当时校歌所描述的：“半似乡村半似城，花木苍翠四时荣”。      0\n",
       "6  經典物理學通常用以闡述日常可觀察尺寸的系統現象，而現代物理學通常用以闡述極端或非常大尺寸、非...      1\n",
       "7           例如，化學元素可以被辨識的最小尺寸是原子物理學或核子物理學探索物質所操作的尺寸。      0\n",
       "8             而粒子物理學操作的尺寸則更為微小，它論述的是基本粒子或由基本粒子組成的粒子。      0\n",
       "9    由於使用大型粒子加速器來產生基本粒子需要非常巨大的能量，所以通常粒子物理學又稱為高能量物理學。      0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_test_new = new_df(X_cn_test)\n",
    "cn_test_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd213dab",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2e31b",
   "metadata": {},
   "source": [
    "Now, I will check the distribution of values in the 'First' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50c030dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2880, 1: 750})\n",
      "Counter({0: 1052, 1: 250})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(cn_train_new['First']))\n",
    "print(Counter(cn_test_new['First']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc47669",
   "metadata": {},
   "source": [
    "It looks like we have an imbalanced dataset. I will use random over sampler to oversample the minority class (the first sentences) for the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f93d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 2880, 0: 2880})\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the random over sampler \n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Resample X, y\n",
    "X_ros_train, y_ros_train = ros.fit_resample(cn_train_new['Sentence'].values.reshape(-1,1), cn_train_new['First'].values.reshape(-1,1))\n",
    "\n",
    "# Check new value distribution \n",
    "print(Counter(y_ros_train))\n",
    "\n",
    "# Reshape the new samples\n",
    "X_ros_train = X_ros_train.flatten()\n",
    "y_ros_train = y_ros_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16554d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1052, 0: 1052})\n"
     ]
    }
   ],
   "source": [
    "# Resample X, y\n",
    "X_ros_test, y_ros_test = ros.fit_resample(cn_test_new['Sentence'].values.reshape(-1,1), cn_test_new['First'].values.reshape(-1,1))\n",
    "\n",
    "# Check new value distribution \n",
    "print(Counter(y_ros_test))\n",
    "\n",
    "# Reshape the new samples\n",
    "X_ros_test = X_ros_test.flatten()\n",
    "y_ros_test = y_ros_test.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9256e6b",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d855b",
   "metadata": {},
   "source": [
    "Now that I have a new, self supervised dataset that has had the minority class oversampled, I can perform a latent semantic analysis on the data to create document term matrices, which I can then train a model on in order to make predictions about whether a sentence is the first in the paragraph or not.\n",
    "\n",
    "I will first try this using Count Vectorizer and then try it using Tf-idf and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9087b6",
   "metadata": {},
   "source": [
    "First, I will create a function that can tokenize Chinese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d251dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Chinese text tokenizer\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "stop_words = ['。', '，']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5ac90",
   "metadata": {},
   "source": [
    "### LSA Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24787039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document term matrix using Count Vectorizer and fit it using the training data.\n",
    "# Then transform the test data.\n",
    "cn_cv = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "cn_train_dtm = cn_cv.fit_transform(X_ros_train)\n",
    "cn_test_dtm = cn_cv.transform(X_ros_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7694410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrices into latent semantic analyses.\n",
    "svd_cv = TruncatedSVD(n_components=75)\n",
    "cv_train_lsa = svd_cv.fit_transform(cn_train_dtm)\n",
    "cv_test_lsa = svd_cv.transform(cn_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573c776",
   "metadata": {},
   "source": [
    "Now, I will use a Logistic Regression model to train a model that takes as inputs the latent semnatic analysis and predicts whether or not a sentence is the first in the paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aec8c550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model.\n",
    "cv_lr = LogisticRegression()\n",
    "cv_lr.fit(cv_train_lsa, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "25348d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6456597222222222\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.74      0.68      2880\n",
      "           1       0.68      0.55      0.61      2880\n",
      "\n",
      "    accuracy                           0.65      5760\n",
      "   macro avg       0.65      0.65      0.64      5760\n",
      "weighted avg       0.65      0.65      0.64      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2134  746]\n",
      " [1295 1585]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_lr_train_pred = cv_lr.predict(cv_train_lsa)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_lr_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_lr_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55529b3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6083650190114068\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.72      0.65      1052\n",
      "           1       0.64      0.50      0.56      1052\n",
      "\n",
      "    accuracy                           0.61      2104\n",
      "   macro avg       0.61      0.61      0.60      2104\n",
      "weighted avg       0.61      0.61      0.60      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[759 293]\n",
      " [531 521]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_lr_test_pred = cv_lr.predict(cv_test_lsa)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_lr_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_lr_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd8003",
   "metadata": {},
   "source": [
    "When generalizing to the test data, F1 scores are 0.64 and 0.39 for non-first sentences and first sentences respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b07cce",
   "metadata": {},
   "source": [
    "### LSA Using Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "354df80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document term matrix using Tf-idf and fit it using the training data.\n",
    "# Then transform the test data.\n",
    "cn_tf = TfidfVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "tf_train_dtm = cn_tf.fit_transform(X_ros_train)\n",
    "tf_test_dtm = cn_tf.transform(X_ros_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "721aac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrix into latent semantic analysis.\n",
    "svd_tf = TruncatedSVD(n_components=75)\n",
    "tf_train_lsa = svd_tf.fit_transform(tf_train_dtm)\n",
    "tf_test_lsa = svd_tf.transform(tf_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97edb3",
   "metadata": {},
   "source": [
    "Now, I will use a Logistic Regression model to train a model that takes as inputs the latent semnatic analysis and predicts whether or not a sentence is the first in the paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a63e8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model.\n",
    "tf_lr = LogisticRegression()\n",
    "tf_lr.fit(tf_train_lsa, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d57b93bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6899305555555556\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70      2880\n",
      "           1       0.70      0.65      0.68      2880\n",
      "\n",
      "    accuracy                           0.69      5760\n",
      "   macro avg       0.69      0.69      0.69      5760\n",
      "weighted avg       0.69      0.69      0.69      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2092  788]\n",
      " [ 998 1882]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_tf_train_pred = tf_lr.predict(tf_train_lsa)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_tf_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_tf_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_tf_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ccaec9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6216730038022814\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.63      0.62      1052\n",
      "           1       0.62      0.62      0.62      1052\n",
      "\n",
      "    accuracy                           0.62      2104\n",
      "   macro avg       0.62      0.62      0.62      2104\n",
      "weighted avg       0.62      0.62      0.62      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[661 391]\n",
      " [405 647]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_tf_test_pred = tf_lr.predict(tf_test_lsa)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_tf_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_tf_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_tf_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82d12cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data</th>\n",
       "      <th>Test Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.645660</td>\n",
       "      <td>0.608365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.608329</td>\n",
       "      <td>0.558414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.678198</td>\n",
       "      <td>0.619139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Training Data  Test Data\n",
       "Count Vectorizer Accuracy       0.645660   0.608365\n",
       "                 F1 Score       0.608329   0.558414\n",
       "Tf-idf           Accuracy       0.689931   0.621673\n",
       "                 F1 Score       0.678198   0.619139"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies and f1 scores for training and test data.\n",
    "data1 = [[accuracy_score(y_ros_train, y_lsa_lr_train_pred), accuracy_score(y_ros_test, y_lsa_lr_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_lr_train_pred), f1_score(y_ros_test, y_lsa_lr_test_pred)],\n",
    "    [accuracy_score(y_ros_train, y_lsa_tf_train_pred), accuracy_score(y_ros_test, y_lsa_tf_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_tf_train_pred), f1_score(y_ros_test, y_lsa_tf_test_pred)]\n",
    "       ]\n",
    "\n",
    "accuracy_f1 = pd.DataFrame(data1, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Accuracy', 'F1 Score', 'Accuracy', 'F1 Score']],\n",
    "                          columns = ['Training Data','Test Data'])\n",
    "accuracy_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d52e1",
   "metadata": {},
   "source": [
    "Tf-idf with logistic regression appears to be the better model. Next, I will further optimize it to prevent overfitting while increasing the accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9830c6",
   "metadata": {},
   "source": [
    "### SVD with 100 components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b2bef",
   "metadata": {},
   "source": [
    "First, I will try a few different values for SVD components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c1e159d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrices into latent semantic analyses.\n",
    "svd_tf_100 = TruncatedSVD(n_components=100)\n",
    "tf_train_lsa_100 = svd_tf_100.fit_transform(cn_train_dtm)\n",
    "tf_test_lsa_100 = svd_tf_100.transform(cn_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f5c952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to the Tf-idf with Logistic regression model created earlier.\n",
    "tf_lr.fit(tf_train_lsa_100, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cbf20291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6595486111111111\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69      2880\n",
      "           1       0.69      0.58      0.63      2880\n",
      "\n",
      "    accuracy                           0.66      5760\n",
      "   macro avg       0.66      0.66      0.66      5760\n",
      "weighted avg       0.66      0.66      0.66      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2139  741]\n",
      " [1220 1660]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_100_train_pred = tf_lr.predict(tf_train_lsa_100)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_100_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_100_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_100_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "497dff0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6140684410646388\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      1052\n",
      "           1       0.65      0.50      0.57      1052\n",
      "\n",
      "    accuracy                           0.61      2104\n",
      "   macro avg       0.62      0.61      0.61      2104\n",
      "weighted avg       0.62      0.61      0.61      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[764 288]\n",
      " [524 528]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_100_test_pred = tf_lr.predict(tf_test_lsa_100)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_100_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_100_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_100_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0010c5",
   "metadata": {},
   "source": [
    "### SVD with 50 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6219ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrices into latent semantic analyses.\n",
    "svd_tf_50 = TruncatedSVD(n_components=50)\n",
    "tf_train_lsa_50 = svd_tf_50.fit_transform(tf_train_dtm)\n",
    "tf_test_lsa_50 = svd_tf_50.transform(tf_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "04772467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_lr.fit(tf_train_lsa_50, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b73896aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6623263888888888\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68      2880\n",
      "           1       0.68      0.61      0.65      2880\n",
      "\n",
      "    accuracy                           0.66      5760\n",
      "   macro avg       0.66      0.66      0.66      5760\n",
      "weighted avg       0.66      0.66      0.66      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2047  833]\n",
      " [1112 1768]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_50_train_pred = tf_lr.predict(tf_train_lsa_50)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_50_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_50_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_50_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e44106d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6297528517110266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62      1052\n",
      "           1       0.62      0.65      0.64      1052\n",
      "\n",
      "    accuracy                           0.63      2104\n",
      "   macro avg       0.63      0.63      0.63      2104\n",
      "weighted avg       0.63      0.63      0.63      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[636 416]\n",
      " [363 689]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_50_test_pred = tf_lr.predict(tf_test_lsa_50)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_50_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_50_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_50_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25846e4",
   "metadata": {},
   "source": [
    "### SVD with 25 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "221bf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrices into latent semantic analyses.\n",
    "svd_tf_25 = TruncatedSVD(n_components=25)\n",
    "tf_train_lsa_25 = svd_tf_25.fit_transform(tf_train_dtm)\n",
    "tf_test_lsa_25 = svd_tf_25.transform(tf_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "aea4dea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_lr.fit(tf_train_lsa_25, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92212a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6657986111111112\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      2880\n",
      "           1       0.68      0.64      0.66      2880\n",
      "\n",
      "    accuracy                           0.67      5760\n",
      "   macro avg       0.67      0.67      0.67      5760\n",
      "weighted avg       0.67      0.67      0.67      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2001  879]\n",
      " [1046 1834]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_25_train_pred = tf_lr.predict(tf_train_lsa_25)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_25_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_25_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_25_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "73a1e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6188212927756654\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.59      0.61      1052\n",
      "           1       0.61      0.65      0.63      1052\n",
      "\n",
      "    accuracy                           0.62      2104\n",
      "   macro avg       0.62      0.62      0.62      2104\n",
      "weighted avg       0.62      0.62      0.62      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[618 434]\n",
      " [368 684]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_25_test_pred = tf_lr.predict(tf_test_lsa_25)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_25_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_25_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_25_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e90e60e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data</th>\n",
       "      <th>Test Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">25 components</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.665799</td>\n",
       "      <td>0.618821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.655820</td>\n",
       "      <td>0.630415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50 components</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.662326</td>\n",
       "      <td>0.629753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.645138</td>\n",
       "      <td>0.638850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">75 components</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.678198</td>\n",
       "      <td>0.619139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">100 components</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.659549</td>\n",
       "      <td>0.614068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.628669</td>\n",
       "      <td>0.565310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Training Data  Test Data\n",
       "25 components  Accuracy       0.665799   0.618821\n",
       "               F1 Score       0.655820   0.630415\n",
       "50 components  Accuracy       0.662326   0.629753\n",
       "               F1 Score       0.645138   0.638850\n",
       "75 components  Accuracy       0.689931   0.621673\n",
       "               F1 Score       0.678198   0.619139\n",
       "100 components Accuracy       0.659549   0.614068\n",
       "               F1 Score       0.628669   0.565310"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies and f1 scores for training and test data.\n",
    "data_svd = [[accuracy_score(y_ros_train, y_lsa_25_train_pred), accuracy_score(y_ros_test, y_lsa_25_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_25_train_pred), f1_score(y_ros_test, y_lsa_25_test_pred)],\n",
    "    [accuracy_score(y_ros_train, y_lsa_50_train_pred), accuracy_score(y_ros_test, y_lsa_50_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_50_train_pred), f1_score(y_ros_test, y_lsa_50_test_pred)],\n",
    "            [accuracy_score(y_ros_train, y_lsa_tf_train_pred), accuracy_score(y_ros_test, y_lsa_tf_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_tf_train_pred), f1_score(y_ros_test, y_lsa_tf_test_pred)],\n",
    "    [accuracy_score(y_ros_train, y_lsa_100_train_pred), accuracy_score(y_ros_test, y_lsa_100_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_100_train_pred), f1_score(y_ros_test, y_lsa_100_test_pred)]\n",
    "       ]\n",
    "\n",
    "tf_svd_25_50_75_100 = pd.DataFrame(data_svd, \n",
    "                           index = [['25 components', '25 components','50 components', '50 components','75 components','75 components','100 components','100 components'],\n",
    "                                   ['Accuracy', 'F1 Score','Accuracy', 'F1 Score', 'Accuracy', 'F1 Score','Accuracy', 'F1 Score']],\n",
    "                          columns = ['Training Data','Test Data'])\n",
    "tf_svd_25_50_75_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f31d86",
   "metadata": {},
   "source": [
    "It appears that reducing the number of components to 50 helps improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fe0a7",
   "metadata": {},
   "source": [
    "### Optimizing for F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a51346",
   "metadata": {},
   "source": [
    "For the last part of this project, I will train a model that optimizes for F1 score instead of accuracy to see if the F1 score can be improved any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5f9cc6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=1000),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10, 100]}, scoring='f1')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model using the Tf-idf LSA \n",
    "# training data by using GridSearch and optimizing for f1 score.\n",
    "tf_lr_f1 = LogisticRegression(max_iter=1000)\n",
    "gs = GridSearchCV(tf_lr_f1, param_grid={'C':[0.01, 0.1, 1, 10, 100]}, scoring='f1')\n",
    "gs.fit(tf_train_lsa_50, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "08bbeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6623263888888888\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68      2880\n",
      "           1       0.68      0.61      0.65      2880\n",
      "\n",
      "    accuracy                           0.66      5760\n",
      "   macro avg       0.66      0.66      0.66      5760\n",
      "weighted avg       0.66      0.66      0.66      5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2047  833]\n",
      " [1112 1768]]\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "gs_train_pred = gs.predict(tf_train_lsa_50)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, gs_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, gs_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, gs_train_pred))\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ac0cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6297528517110266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62      1052\n",
      "           1       0.62      0.65      0.64      1052\n",
      "\n",
      "    accuracy                           0.63      2104\n",
      "   macro avg       0.63      0.63      0.63      2104\n",
      "weighted avg       0.63      0.63      0.63      2104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[636 416]\n",
      " [363 689]]\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "gs_test_pred = gs.predict(tf_test_lsa_50)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, gs_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, gs_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, gs_test_pred))\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a7dc5f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data</th>\n",
       "      <th>Test Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Un-optimized</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.662326</td>\n",
       "      <td>0.629753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.645138</td>\n",
       "      <td>0.638850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Optimized for F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.662326</td>\n",
       "      <td>0.629753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.645138</td>\n",
       "      <td>0.638850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Training Data  Test Data\n",
       "Un-optimized     Accuracy       0.662326   0.629753\n",
       "                 F1 Score       0.645138   0.638850\n",
       "Optimized for F1 Accuracy       0.662326   0.629753\n",
       "                 F1 Score       0.645138   0.638850"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies and f1 scores for training and test data.\n",
    "data1 = [[accuracy_score(y_ros_train, y_lsa_50_train_pred), accuracy_score(y_ros_test, y_lsa_50_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_50_train_pred), f1_score(y_ros_test, y_lsa_50_test_pred)],\n",
    "            [accuracy_score(y_ros_train, gs_train_pred), accuracy_score(y_ros_test, gs_test_pred)], \n",
    "        [f1_score(y_ros_train, gs_train_pred), f1_score(y_ros_test, gs_test_pred)]\n",
    "       ]\n",
    "\n",
    "tf_optimized = pd.DataFrame(data1, \n",
    "                           index = [['Un-optimized', 'Un-optimized','Optimized for F1','Optimized for F1'],\n",
    "                                    ['Accuracy', 'F1 Score', 'Accuracy', 'F1 Score']],\n",
    "                          columns = ['Training Data','Test Data'])\n",
    "tf_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd30e33",
   "metadata": {},
   "source": [
    "After optimizing C for F1 score, I found that the best parameter for C was 1, which is the same as in the previous model. That's why all of the accuracy and F1 values are the same. \n",
    "\n",
    "The best model for predicting whether a sentence is the first in its paragraph was Logistic Regression using Tf-idf with LSA with 50 components for SVD and 1 for C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1b2b2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7950c2e",
   "metadata": {},
   "source": [
    "Given that the challenge was to predict whether a sentence was first in a paragraph or not, that I found a model which could predict this with 66% accuracy is pretty good. This shows that the Tf-idf vectorizer and latent semantic analysis were able to derive enough meaning from the data so that a Logistic Regression estimator could make predictions with a decent level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a54cc86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'accuracy_df' (DataFrame)\n",
      "Stored 'cross_val_scores' (DataFrame)\n",
      "Stored 'accuracy_f1' (DataFrame)\n",
      "Stored 'tf_svd_25_50_75_100' (DataFrame)\n",
      "Stored 'tf_optimized' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Store variables for use in other notebooks\n",
    "%store accuracy_df\n",
    "%store cross_val_scores\n",
    "%store accuracy_f1\n",
    "%store tf_svd_25_50_75_100\n",
    "%store tf_optimized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
