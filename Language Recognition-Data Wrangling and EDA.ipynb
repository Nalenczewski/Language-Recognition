{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e7fa5c",
   "metadata": {},
   "source": [
    "# Language Recognition - Data Wrangling and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874a318",
   "metadata": {},
   "source": [
    "The goal of this project is to predict one of 22 different languages based on its text as input. I aim to do this by creating eight different models: Logistic Regression and Naive Bayes implementations with each model incorporating Count Vectorizer, Tf-idf, word embeddings, and document vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c96202",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ece9db",
   "metadata": {},
   "source": [
    "I will start by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae724d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323491",
   "metadata": {},
   "source": [
    "### Import and display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a99bb2",
   "metadata": {},
   "source": [
    "This data was taken from the Kaggle language identification data set (https://www.kaggle.com/datasets/zarajamshaid/language-identification-datasst). The data was taken from WiLi-2018 wikipedia dataset, which contains 235,000 paragraphs of 235 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca0f4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsutinalar i̇ngilizce tsuutina kanadada albert...</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>müller mox figura centralis circulorum doctoru...</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>برقی بار electric charge تمام زیرجوہری ذرات کی...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "5  エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...  Japanese\n",
       "6  tsutinalar i̇ngilizce tsuutina kanadada albert...   Turkish\n",
       "7  müller mox figura centralis circulorum doctoru...     Latin\n",
       "8  برقی بار electric charge تمام زیرجوہری ذرات کی...      Urdu\n",
       "9  シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...  Japanese"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and display the data\n",
    "df = pd.read_csv('language.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93a5ed",
   "metadata": {},
   "source": [
    "The d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ece79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the shape of the data.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc76cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tamil         1000\n",
       "Swedish       1000\n",
       "Latin         1000\n",
       "Korean        1000\n",
       "Indonesian    1000\n",
       "Spanish       1000\n",
       "Hindi         1000\n",
       "English       1000\n",
       "Estonian      1000\n",
       "Chinese       1000\n",
       "Turkish       1000\n",
       "Pushto        1000\n",
       "Thai          1000\n",
       "Urdu          1000\n",
       "Dutch         1000\n",
       "Japanese      1000\n",
       "French        1000\n",
       "Portugese     1000\n",
       "Romanian      1000\n",
       "Russian       1000\n",
       "Persian       1000\n",
       "Arabic        1000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the data in more detail.\n",
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03598ace",
   "metadata": {},
   "source": [
    "Based on the initial inspection of the data we see it consists of 1,000 examples each of 22 languages. This is plenty of data for my purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538937b0",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b885e",
   "metadata": {},
   "source": [
    "Next, I will create the X and y variables for modeling. Since the y variable will be the same for all models, I will only need to create it once. However, since I am using four different methods to create four sets of feature vectors, I need to create four different Xs. I will create each X variable as I go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857af1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = df['Text']\n",
    "y = df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bdcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels into numbers.\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f0d44",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizer + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d534b",
   "metadata": {},
   "source": [
    "First I will create a Count Vectorizer Logistic Regression model. The first step is to create some sparse matrices for the X variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce62345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Count Vectorizer and set X_cv equal to the transformed data\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7d1f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6f0fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the number of tokens in the first example.\n",
    "len(set(X[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa82bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(X_cv, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0038b8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17600, 277720), (4400, 277720))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the shape of the data\n",
    "X_cv_train.shape, X_cv_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aaa6612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of CountVectorizer\n",
    "cv_lr = LogisticRegression(max_iter = 10000, C = 0.1)\n",
    "cv_lr.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3846f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.99375\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       821\n",
      "           1       0.98      0.99      0.99       797\n",
      "           2       1.00      1.00      1.00       806\n",
      "           3       0.96      1.00      0.98       793\n",
      "           4       1.00      0.99      0.99       813\n",
      "           5       0.99      1.00      0.99       786\n",
      "           6       1.00      0.99      1.00       786\n",
      "           7       1.00      0.99      1.00       813\n",
      "           8       0.96      0.99      0.97       817\n",
      "           9       1.00      1.00      1.00       815\n",
      "          10       0.98      0.99      0.99       786\n",
      "          11       1.00      1.00      1.00       772\n",
      "          12       1.00      0.99      0.99       800\n",
      "          13       1.00      0.99      1.00       807\n",
      "          14       1.00      1.00      1.00       814\n",
      "          15       1.00      1.00      1.00       794\n",
      "          16       1.00      0.99      0.99       796\n",
      "          17       1.00      1.00      1.00       800\n",
      "          18       1.00      0.99      1.00       816\n",
      "          19       1.00      0.99      1.00       779\n",
      "          20       1.00      0.99      1.00       795\n",
      "          21       1.00      0.99      0.99       794\n",
      "\n",
      "    accuracy                           0.99     17600\n",
      "   macro avg       0.99      0.99      0.99     17600\n",
      "weighted avg       0.99      0.99      0.99     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_lr_train_pred = cv_lr.predict(X_cv_train)\n",
    "print('Accuracy Score: ', accuracy_score(y_cv_train, y_cv_lr_train_pred))\n",
    "print('Classification Report: ', classification_report(y_cv_train, y_cv_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9feaf4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9336363636363636\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       179\n",
      "           1       0.73      0.53      0.62       203\n",
      "           2       1.00      0.97      0.98       194\n",
      "           3       0.88      0.95      0.91       207\n",
      "           4       0.98      0.91      0.95       187\n",
      "           5       0.97      0.97      0.97       214\n",
      "           6       1.00      0.98      0.99       214\n",
      "           7       0.99      0.95      0.97       187\n",
      "           8       0.46      0.93      0.62       183\n",
      "           9       1.00      0.88      0.94       185\n",
      "          10       0.95      0.93      0.94       214\n",
      "          11       1.00      0.97      0.98       228\n",
      "          12       0.97      0.96      0.97       200\n",
      "          13       1.00      0.95      0.98       193\n",
      "          14       0.99      0.99      0.99       186\n",
      "          15       0.99      0.93      0.96       206\n",
      "          16       1.00      0.96      0.98       204\n",
      "          17       1.00      0.98      0.99       200\n",
      "          18       1.00      0.99      0.99       184\n",
      "          19       1.00      0.95      0.98       221\n",
      "          20       1.00      0.93      0.96       205\n",
      "          21       1.00      0.97      0.98       206\n",
      "\n",
      "    accuracy                           0.93      4400\n",
      "   macro avg       0.95      0.93      0.94      4400\n",
      "weighted avg       0.95      0.93      0.94      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_lr_test_pred = cv_lr.predict(X_cv_test)\n",
    "print('Accuracy Score: ',accuracy_score(y_cv_test, y_cv_lr_test_pred))\n",
    "print('Classification Report: ', classification_report(y_cv_test, y_cv_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9727675",
   "metadata": {},
   "source": [
    "With 99% accuracy on the test data and 94% accuracy on the test data, we have a fairly accurate model on our hands that generalizes well. Not bad for a first attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39a21a",
   "metadata": {},
   "source": [
    "### Model 2: CountVectorizer + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4adc4",
   "metadata": {},
   "source": [
    "Now I will create a CountVectorizer with Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7b2d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of CountVectorizer\n",
    "cv_nb = MultinomialNB(alpha=0.1)\n",
    "cv_nb.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0454eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9918181818181818\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       821\n",
      "           1       1.00      0.99      0.99       797\n",
      "           2       1.00      1.00      1.00       806\n",
      "           3       0.86      1.00      0.92       793\n",
      "           4       1.00      0.99      1.00       813\n",
      "           5       0.99      1.00      0.99       786\n",
      "           6       1.00      0.98      0.99       786\n",
      "           7       1.00      0.99      0.99       813\n",
      "           8       1.00      1.00      1.00       817\n",
      "           9       1.00      1.00      1.00       815\n",
      "          10       1.00      0.98      0.99       786\n",
      "          11       1.00      1.00      1.00       772\n",
      "          12       1.00      0.99      0.99       800\n",
      "          13       1.00      0.97      0.98       807\n",
      "          14       1.00      0.99      1.00       814\n",
      "          15       1.00      0.99      1.00       794\n",
      "          16       1.00      0.99      1.00       796\n",
      "          17       1.00      1.00      1.00       800\n",
      "          18       1.00      0.99      1.00       816\n",
      "          19       1.00      0.99      0.99       779\n",
      "          20       1.00      0.99      1.00       795\n",
      "          21       1.00      0.99      0.99       794\n",
      "\n",
      "    accuracy                           0.99     17600\n",
      "   macro avg       0.99      0.99      0.99     17600\n",
      "weighted avg       0.99      0.99      0.99     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_nb_train_pred = cv_nb.predict(X_cv_train)\n",
    "print('Accuracy score: ', accuracy_score(y_cv_train, y_cv_nb_train_pred))\n",
    "print('Classification report: ', classification_report(y_cv_train, y_cv_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58640f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9556818181818182\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       179\n",
      "           1       0.94      0.54      0.69       203\n",
      "           2       1.00      0.98      0.99       194\n",
      "           3       0.71      1.00      0.83       207\n",
      "           4       0.99      0.96      0.97       187\n",
      "           5       0.95      0.98      0.97       214\n",
      "           6       1.00      0.99      1.00       214\n",
      "           7       0.99      0.98      0.99       187\n",
      "           8       0.67      0.91      0.77       183\n",
      "           9       1.00      0.97      0.98       185\n",
      "          10       0.99      0.93      0.96       214\n",
      "          11       1.00      1.00      1.00       228\n",
      "          12       0.99      0.93      0.96       200\n",
      "          13       1.00      0.95      0.98       193\n",
      "          14       0.99      0.99      0.99       186\n",
      "          15       1.00      0.99      0.99       206\n",
      "          16       1.00      0.98      0.99       204\n",
      "          17       1.00      1.00      1.00       200\n",
      "          18       1.00      0.99      0.99       184\n",
      "          19       1.00      0.99      0.99       221\n",
      "          20       0.99      0.99      0.99       205\n",
      "          21       1.00      0.99      1.00       206\n",
      "\n",
      "    accuracy                           0.96      4400\n",
      "   macro avg       0.96      0.96      0.96      4400\n",
      "weighted avg       0.96      0.96      0.96      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_nb_test_pred = cv_nb.predict(X_cv_test)\n",
    "print('Accuracy score: ', accuracy_score(y_cv_test, y_cv_nb_test_pred))\n",
    "print('Classification report: ', classification_report(y_cv_test, y_cv_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3ce2d",
   "metadata": {},
   "source": [
    "With 99% accuracy on the training data and 95% accuracy on the test data, this model is slightly better than the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c406be",
   "metadata": {},
   "source": [
    "### Model 3: Tf-idf + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b98e22",
   "metadata": {},
   "source": [
    "Now I will build the third model, Tf-idf with Logistic Regression. First we need to transform the text using the Tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdeabd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Tf-idf vectorizer and set X_tf equal to the transformed data\n",
    "tf = TfidfVectorizer()\n",
    "X_tf = tf.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "990c99c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d26701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1880fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of Tf-idf\n",
    "tf_lr = LogisticRegression(max_iter = 10000, C = 0.1)\n",
    "tf_lr.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "356f6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9744886363636364\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       791\n",
      "           1       0.86      0.98      0.92       798\n",
      "           2       1.00      0.97      0.99       808\n",
      "           3       0.79      0.99      0.88       811\n",
      "           4       0.99      0.96      0.98       770\n",
      "           5       0.97      0.99      0.98       808\n",
      "           6       1.00      0.98      0.99       806\n",
      "           7       1.00      0.97      0.98       805\n",
      "           8       0.96      0.95      0.96       795\n",
      "           9       1.00      1.00      1.00       797\n",
      "          10       0.97      0.93      0.95       810\n",
      "          11       1.00      0.98      0.99       793\n",
      "          12       0.99      0.94      0.97       816\n",
      "          13       1.00      0.94      0.97       796\n",
      "          14       1.00      0.98      0.99       802\n",
      "          15       0.99      0.99      0.99       791\n",
      "          16       1.00      0.96      0.98       791\n",
      "          17       1.00      0.99      1.00       803\n",
      "          18       1.00      0.99      0.99       804\n",
      "          19       1.00      0.99      0.99       818\n",
      "          20       1.00      0.98      0.99       789\n",
      "          21       1.00      0.98      0.99       798\n",
      "\n",
      "    accuracy                           0.97     17600\n",
      "   macro avg       0.98      0.97      0.98     17600\n",
      "weighted avg       0.98      0.97      0.98     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_lr_train_pred = tf_lr.predict(X_tf_train)\n",
    "print('Accuracy score: ', accuracy_score(y_tf_train, y_tf_lr_train_pred))\n",
    "print('Classification report: ', classification_report(y_tf_train, y_tf_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e6390e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9545454545454546\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       209\n",
      "           1       0.62      0.98      0.75       202\n",
      "           2       1.00      0.97      0.98       192\n",
      "           3       0.78      0.99      0.87       189\n",
      "           4       0.99      0.93      0.96       230\n",
      "           5       0.96      0.98      0.97       192\n",
      "           6       1.00      0.98      0.99       194\n",
      "           7       1.00      0.99      0.99       195\n",
      "           8       0.96      0.64      0.77       205\n",
      "           9       1.00      0.96      0.98       203\n",
      "          10       0.97      0.92      0.95       190\n",
      "          11       1.00      0.98      0.99       207\n",
      "          12       0.98      0.95      0.96       184\n",
      "          13       1.00      0.96      0.98       204\n",
      "          14       1.00      0.98      0.99       198\n",
      "          15       1.00      0.96      0.98       209\n",
      "          16       1.00      0.97      0.98       209\n",
      "          17       1.00      0.99      1.00       197\n",
      "          18       1.00      0.97      0.99       196\n",
      "          19       1.00      0.98      0.99       182\n",
      "          20       1.00      0.98      0.99       211\n",
      "          21       1.00      0.97      0.98       202\n",
      "\n",
      "    accuracy                           0.95      4400\n",
      "   macro avg       0.97      0.96      0.96      4400\n",
      "weighted avg       0.97      0.95      0.96      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_lr_test_pred = tf_lr.predict(X_tf_test)\n",
    "print('Accuracy score: ', accuracy_score(y_tf_test, y_tf_lr_test_pred))\n",
    "print('Classification report: ', classification_report(y_tf_test, y_tf_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b417fa",
   "metadata": {},
   "source": [
    "With 97% accuracy on the training data and 94% accuracy on the test data, this is a pretty good model. However, it is surprising that overall it didn't do as well as the Count Vectorizer models did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be581f",
   "metadata": {},
   "source": [
    "### Model 4: Tf-idf + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d876b4",
   "metadata": {},
   "source": [
    "I will now create the fourth model: Tf-idf with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "901be54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of Tf-idf\n",
    "tf_nb = MultinomialNB()\n",
    "tf_nb.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "081914a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9832386363636364\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       791\n",
      "           1       1.00      0.95      0.98       798\n",
      "           2       1.00      0.98      0.99       808\n",
      "           3       0.77      1.00      0.87       811\n",
      "           4       1.00      0.98      0.99       770\n",
      "           5       0.96      0.99      0.98       808\n",
      "           6       1.00      0.98      0.99       806\n",
      "           7       1.00      0.98      0.99       805\n",
      "           8       1.00      0.98      0.99       795\n",
      "           9       1.00      1.00      1.00       797\n",
      "          10       1.00      0.95      0.97       810\n",
      "          11       1.00      1.00      1.00       793\n",
      "          12       0.99      0.97      0.98       816\n",
      "          13       1.00      0.96      0.98       796\n",
      "          14       1.00      0.99      0.99       802\n",
      "          15       0.99      0.99      0.99       791\n",
      "          16       1.00      0.98      0.99       791\n",
      "          17       1.00      1.00      1.00       803\n",
      "          18       1.00      0.99      1.00       804\n",
      "          19       1.00      0.99      0.99       818\n",
      "          20       1.00      0.99      0.99       789\n",
      "          21       1.00      0.99      0.99       798\n",
      "\n",
      "    accuracy                           0.98     17600\n",
      "   macro avg       0.99      0.98      0.98     17600\n",
      "weighted avg       0.99      0.98      0.98     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_nb_train_pred = tf_nb.predict(X_tf_train)\n",
    "print('Accuracy score: ', accuracy_score(y_tf_train, y_tf_nb_train_pred))\n",
    "print('Classification report: ', classification_report(y_tf_train, y_tf_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "528b0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9434090909090909\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       209\n",
      "           1       0.98      0.55      0.71       202\n",
      "           2       0.99      0.99      0.99       192\n",
      "           3       0.67      1.00      0.80       189\n",
      "           4       1.00      0.96      0.98       230\n",
      "           5       0.95      0.99      0.97       192\n",
      "           6       0.73      0.98      0.84       194\n",
      "           7       0.98      0.99      0.99       195\n",
      "           8       0.90      0.59      0.71       205\n",
      "           9       1.00      0.97      0.98       203\n",
      "          10       0.99      0.91      0.95       190\n",
      "          11       1.00      1.00      1.00       207\n",
      "          12       0.80      0.96      0.87       184\n",
      "          13       1.00      0.98      0.99       204\n",
      "          14       0.99      0.99      0.99       198\n",
      "          15       1.00      0.98      0.99       209\n",
      "          16       1.00      1.00      1.00       209\n",
      "          17       0.98      1.00      0.99       197\n",
      "          18       1.00      0.98      0.99       196\n",
      "          19       1.00      0.98      0.99       182\n",
      "          20       0.99      0.99      0.99       211\n",
      "          21       1.00      0.98      0.99       202\n",
      "\n",
      "    accuracy                           0.94      4400\n",
      "   macro avg       0.95      0.94      0.94      4400\n",
      "weighted avg       0.95      0.94      0.94      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_nb_test_pred = tf_nb.predict(X_tf_test)\n",
    "print('Accuracy score: ', accuracy_score(y_tf_test, y_tf_nb_test_pred))\n",
    "print('Classification report: ', classification_report(y_tf_test, y_tf_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7645c",
   "metadata": {},
   "source": [
    "With an accuracy score of 98% on the training data and 95% on the test data this is a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ecad91",
   "metadata": {},
   "source": [
    "### Hyperparameter Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87449e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data Accuracy</th>\n",
       "      <th>Test Data Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.933636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.991818</td>\n",
       "      <td>0.955682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.974489</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.983239</td>\n",
       "      <td>0.943409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Training Data Accuracy  \\\n",
       "Count Vectorizer Logistic Regression                0.993750   \n",
       "                 Naive Bayes                        0.991818   \n",
       "Tf-idf           Logistic Regression                0.974489   \n",
       "                 Naive Bayes                        0.983239   \n",
       "\n",
       "                                      Test Data Accuracy  \n",
       "Count Vectorizer Logistic Regression            0.933636  \n",
       "                 Naive Bayes                    0.955682  \n",
       "Tf-idf           Logistic Regression            0.954545  \n",
       "                 Naive Bayes                    0.943409  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies of the eight models for training and test data.\n",
    "data = [[accuracy_score(y_cv_train, y_cv_lr_train_pred), accuracy_score(y_cv_test, y_cv_lr_test_pred)], \n",
    "        [accuracy_score(y_cv_train, y_cv_nb_train_pred), accuracy_score(y_cv_test, y_cv_nb_test_pred)],\n",
    "        [accuracy_score(y_tf_train, y_tf_lr_train_pred), accuracy_score(y_tf_test, y_tf_lr_test_pred)], \n",
    "        [accuracy_score(y_tf_train, y_tf_nb_train_pred), accuracy_score(y_tf_test, y_tf_nb_test_pred)]\n",
    "       ]\n",
    "\n",
    "accuracy_df = pd.DataFrame(data, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Logistic Regression', 'Naive Bayes', 'Logistic Regression','Naive Bayes']],\n",
    "                          columns = ['Training Data Accuracy','Test Data Accuracy'])\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6b29f",
   "metadata": {},
   "source": [
    "From the data we can see that in terms of accuracy, the best model is Count Vectorizer with Naieve Bayes. I wonder why a Count Vectorizer model is better than Tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc6668",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775194c",
   "metadata": {},
   "source": [
    "In the next section, I will use the models I have created to predict the langauge of a given paragraph of text. The text has been gathered from different websites in various languages. In total, there are 22 paragraphs, one for each langauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a73e70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8a1dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for each langauge\n",
    "# Estonian. Source: https://uueduudised.ee/uudis/eesti/ekre-ettepanek-homofilmifestivali-raha-ukraina-kultuuriseltsile-anda-ei-leidnud-rakveres-toetust/\n",
    "a = \"Rakvere linnavolikokku kuuluvates Eesti Konservatiivse Rahvaerakonna saadikutes tekitas küsimusi homofilmifestivali Festheart rahastamine ajal, mil Ukrainas käib sõda ja selle asemel võiks linna eelarves homopropagandale eraldatava kultuurirahaga toetada pigem Ukraina kultuuriseltsi.\"\n",
    "\n",
    "# Swedish. Source: https://www.svt.se/sport/ishockey/mallost-efter-forsta-perioden-i-odesmatchen\n",
    "b = \"Grabbarna känns verkligen laddade för uppgiften, men det är 40 långa minuter kvar, sa Djurgårdens Sebastian Strandberg i C Mores sändning efter de första 20 minuterna. Halvvägs in i ångestmatchen tog Timrå ledningen med 1-0 genom Robin Hanzl, som styrde in matchens första mål, innan Ty Rattie, 56 sekunder senare, utökade till 2-0. Hanzl blev också tvåmålsskytt när Djurgården gav bort pucken i egen zon och släppte in ett tredje mål.\"\n",
    "\n",
    "# Thai. Source: https://nlovecooking.com/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3%E0%B9%84%E0%B8%97%E0%B8%A2-2/\n",
    "c = \"คุณค่าของอาหารไทยด้านวัฒนธรรม การถ่ายทอดความรู้ด้านการทำอาหารใน อาหารไทย นั้น แสดงถึงภูมิปัญญาของคนไทย และ วัฒนธรรมด้านอาหารของคนไทย บ่งบอกถึงความเจริญของชนชาตินั้นๆ อาหารไทย มีเอกลักษณ์ที่แตกต่างจากอาหารของชนชาติอื่นๆ สามารถปรับปรุงรสชาติให้เข้ากับคนุกชาติได้ จึงแสดงถึงคุณค่าของอาหารไทย ที่ทำให้คนทั่วโลกยอมรับ\"\n",
    "\n",
    "# Tamil. Source: https://artsandculture.google.com/entity/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AE%B0%E0%AF%8D-%E0%AE%B5%E0%AE%B0%E0%AE%B2%E0%AE%BE%E0%AE%B1%E0%AF%81/g11cls_rl0p?hl=ta\n",
    "d = \"தமிழர் மத்திய ஆசியா, வட இந்தியா நிலப்பரப்புகளில் இருந்து காலப்போக்கில் தென் இந்தியா வந்தனர் என்பது மற்றைய கருதுகோள். எப்படி இருப்பினும் தமிழர் இனம் தொன்மையான மக்கள் இனங்களில் ஒன்று. தமிழர்களின் தோற்றம் மற்ற திராவிடர்களைப் போலவே இன்னும் தெளிவாக அறியப்படவில்லை.\"\n",
    "\n",
    "# Dutch. Source: https://www.stuivengalederwaren.nl/leukste-hollandse-tassen/\n",
    "e = \"Berba staat vooral bekend om de zachte leren tassen en bijpassende portemonnees. En met de vele vakjes en een lange schouderbanden sluiten de tassen én portemonnees perfect aan bij de wensen van de Hollandse vrouw (en man!). Zo heb je met Berba dé ideale combinatie van schoonheid en functionaliteit.\"\n",
    "\n",
    "# Japanese. Source: https://twitter.com/twitterjp/status/923671036758958080\n",
    "f = \"いつも、そして何年もの間、Twitterをご利用いただきありがとうございます。おかげさまで日本での月間利用者数が4500万を超えました。安心してサービスをご利用いただけますように、一層の努力を行います。引き続きのご指導、ご支援のほど、よろしくお願い申し上げます\"\n",
    "\n",
    "# Turkish. Source: https://www.haberturk.com/seren-serengil-e-annesi-nevin-serengil-den-isyan-3396288-magazin\n",
    "g = \"Kimi varlıkla imtihan edilir, kimi yoklukla... Kimi hastalıkla imtihan edilir, kimi sağlıkla... Ama evlatla imtihan edilmek imtihanların en zorudur. Çünkü canını yakan yine kendi canındır. Bin parçaya da bölünürsün ama yine de nefret edemezsin. Rabbim hiç kimseyi evlatlarıyla imtihan etmesin.\"\n",
    "\n",
    "# Latin. Source: https://www.lipsum.com/\n",
    "h = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "# Urdu. Source: https://www.urdunews.com/node/658036\n",
    "i = \"انہوں نے کہا کہ پاکستان میں رجسٹر اور غیر رجسٹرڈ افغان مہاجرین کی تعداد 40 لاکھ کے لگ بھگ ہے۔آرمی چیف نے مشرقی سرحد کی صورتحال پر کہا کہ لائن آف کنٹرول پر حالات بہتر ہیں اور وہاں بسنے والے شہریوں کی زندگی میں امن آیا ہے۔ان کا کہنا تھا کہ انڈین سپر سونک میزائل کے پاکستان میں گرنے کا واقعہ انتہائی تشویشناک ہے۔ عالمی برادری اس کا نوٹس لے گی کیونکہ اس سے یہاں عام شہریوں کا جانی نقصان بھی ہو سکتا تھا جبکہ اس میزائل کے راستے میں آنے والا کوئی مسافر طیارہ بھی نشانہ بن سکتا تھا۔\"\n",
    "\n",
    "# Indonesian. Source: https://news.detik.com/berita/d-6013602/ingat-13-lokasi-di-jakarta-ditutup-jelang-sahur-pukul-0100-0500-wib\n",
    "j = \"Filterisasi mengantisipasi sahur on the road atau SOTR dilakukan Polda Metro Jaya di wilayah DKI Jakarta selama bulan Ramadan. Perlu diingat, total ada 13 lokasi yang diberlakukan filterisasi pada jam-jam menjelang sahur.\"\n",
    "\n",
    "# Protugese. Source: https://www.dn.pt/internacional/ucrania-acusa-tropas-russas-de-abrirem-fogo-contra-manifestantes-pacificos-14737367.html\n",
    "k = \"'Hoje em Energodar, os moradores da cidade reuniram-se de novo manifestando-se em apoio da Ucrânia e cantando o hino nacional', postou na rede social Facebook a responsável pelos Direitos Humanos no Parlamento ucraniano, Lyoudmyla Denisova.\"\n",
    "\n",
    "# French. Source: https://www.francetvinfo.fr/elections/presidentielle/presidentielle-2022-ces-12-millions-de-francais-encore-indecis_5059294.html\n",
    "l = \"Le 10 avril se tiendra le premier tour de l'élection présidentielle. Vendredi 1er avril, 37 % des électeurs ne savent toujours pas pour qui ils vont voter. Ces indécis sont des personnes qui sont certaines d'aller voter, mais qui peuvent changer d'avis. Un citoyen hésite ainsi entre Yannick Jadot (EELV) et Emmanuel Macron (LREM). Une autre dit se laisser encore quelques jours pour consulter les programmes. Près de 6 sur 10 électeurs de Yannick Jadot, Anne Hidalgo (PS) et Fabien Roussel (PCF) sont indécis.\"\n",
    "\n",
    "# Chinese. Source: https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\n",
    "m = \"1949年，以毛泽东主席为领袖的中国共产党领导中国人民解放军在内战中取得优势，实际控制中国大陆，同年10月1日宣布建立中华人民共和国以及中央人民政府，与迁至台湾地区的中华民国政府形成至今的台海现状格局。中华人民共和国成立初期遵循和平共处五项原则的外交政策，1971年在联合国取得了原属于中华民国的中国代表权及其联合国安理会常任理事国席位，并陆续加入部分联合国其他专门机构。而后广泛参与例如国际奥委会、亚太经合组织、二十国集团、世界贸易组织等重要国际组织，并成为上海合作组织、金砖国家、一带一路、亚洲基础设施投资银行、区域全面经济伙伴关系协定等国际合作组织项目的发起国和创始国。据皮尤研究中心的调查，随着国际影响力的增强，中华人民共和国已被许多国家、组织视为世界经济的重要支柱与潜在超级大国之一[41][42][43]。\"\n",
    "\n",
    "# Korean. Source: https://news.kbs.co.kr/news/view.do?ncd=5430540\n",
    "n = \"호남 출신인 한 전 총리는 경제 관료 출신으로, 김대중 정부에서 청와대 경제수석, 노무현 정부에서 국무총리를 지냈고, 이명박 정부에서 주미 대사를, 박근혜 정부에서는 무역협회장을 역임했습니다. 가장 중요한 건 경제라던 윤 당선인은 한 전 총리에 대해 '통합형 총리'에 맞고, 외교와 통상, 경제 전문가로서의 경륜을 높이 사고 있다고 말한 것으로 전해졌습니다. 2007년 총리 후보자로 국회 인사청문회를 통과했던 만큼, 민주당이 다수인 국회에서의 임명 동의 등 여러 측면을 고려한 인선이란 분석도 나옵니다.\"\n",
    "\n",
    "# Hindi. Source: https://www.bbc.com/hindi/india-60964637\n",
    "o = \"भारत दौरे पर आए नेपाल के प्रधानमंत्री शेर बहादुर देउबा की शनिवार को प्रधानमंत्री नरेंद्र मोदी समेत कई महत्वपूर्ण नेताओं से मुलाकात हुई. साथ ही भारत और नेपाल ने शनिवार को सीमा पार रेलवे नेटवर्क समेत कई विकास परियोजनाओं का उद्घाटन किया. इस मौके पर नेपाल के प्रधानमंत्री शेर बहादुर देउबा ने कहा कि दोनों देशों के बीच चल रहे सीमा विवाद को सुलझाने के लिए कोई साझा व्यवस्था बने.\"\n",
    "\n",
    "# Spanish. Source: https://cnnespanol.cnn.com/2022/04/02/analisis-putin-esta-cometiendo-los-mismos-errores-que-condenaron-a-hitler-trax/\n",
    "p = \"Pero los tanques rusos se han visto obstaculizados por otra razón sorprendente: la falta de combustible. La falta de combustible es parte de un problema mayor. El ejército ruso, del que alguna vez se alardeó se ha estancado en Ucrania no solo por la feroz resistencia, sino por algo más prosaico: la logística.\"\n",
    "\n",
    "# Pushto. Source: https://www.bbc.com/pashto/world-60909321\n",
    "q = \"ملګري ملتونه وايي نژدې دوه ميلیونه اوکرايني ماشومان اوس د روسيې له بمبارۍ ګاونډیو هېوادونو ته تښتېدلي دي. يونيسېف او د بشري مرستو نورو ټولنو خبرداری ورکړی، دا ماشومان یې له خپلو ميندو او نورو ښځينه اوکراينيو کډوالو سره د قاچاق او ناوړه ګټې اخيستو لوړې کچې خطر سره مخامخ دي.\"\n",
    "\n",
    "# Persian. Source: https://www.bbc.com/persian/afghanistan-60966238\n",
    "r = \"گزارش‌های قبلا به نقل از طالبان طالبان منتشر شده بود که این گروه برای آزادی مارک فرریکس خواستار رهایی یک افغان به نام بشیر نورزی شده بوده است که در حال گذراندن محکومیت حبس ابد به جرم قاچاق مواد مخدر در ایالات متحده است.\"\n",
    "\n",
    "# Romanian. Source: https://www.digi24.ro/stiri/externe/sua-trimite-ucrainei-echipament-de-protectie-in-caz-de-atacuri-chimice-zelenski-rusii-planuiesc-atacuri-puternice-in-donbas-si-harkov-1891921\n",
    "s = \"Președintele ucrainean Volodimir Zelenski spune că retragerea trupelor rusești din nordul țării este „înceată dar vizibilă”. Acesta avertizează însă ucrainenii că vor urma „lupte grele” în estul țării, în zonele Donbas și Harkov. Peste 3.000 de oameni au reușit să părăsească orașul-port Mariupol, mai spune președintele Ucrainei. Între timp, SUA ajută țara pentru posibile atacuri chimice, trimițând echipament personal de protecție. De asemenea, Pentagonul va oferi Ucrainei un ajutor militar suplimentar de până la 300 de milioane de dolari.\"\n",
    "\n",
    "# Russian. Source: https://ria.ru/20220402/protesty-1781464774.html\n",
    "t = \"Таким образом, расходы британцев на энергию вырастут в среднем на 700 фунтов в год и составят около двух тысяч. Из-за этого годовая инфляция в феврале достигла в Британии рекордного за 30 лет уровня — 6,2 процента.\"\n",
    "\n",
    "# English. Source: https://www.wsj.com/articles/tesla-deliveries-rose-in-quarter-elon-musk-calls-exceptionally-difficult-11648917258?mod=hp_lead_pos2\n",
    "u = \"Tesla Inc. vehicle deliveries rose in the first quarter, but missed Wall Street expectations as the company struggled with global supply-chain disruptions and a brief Covid-19 shutdown at its Shanghai factory. This was an *exceptionally* difficult quarter due to supply chain interruptions & China zero Covid policy,” Tesla Chief Executive Elon Musk tweeted Saturday morning. Tesla employees and key suppliers 'saved the day,' he added.\"\n",
    "\n",
    "# Arabic.\n",
    "v = \"وقال المتحدث باسم الوزارة أحمد الصحاف، إن 'الوزير فؤاد حسين استقبل اليوم سفراء مجموعة G7 المعتمدين لدى العراق، واستعرض تفاصيل وأبعاد زيارته المرتقبة إلى موسكو ووارسو ضمن مجموعة الاتصال العربية على المستوى وزارء في جامعة الدول العربية لمتابعة وإجراء المشاورات والاتصالات اللازمة مع الأطراف المعنية بالأزمة الروسية-الأوكرانية بهدف المساهمة في إيجاد الحلول الدبلوماسية للازمة وإنهاء الحرب القائم'.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cfa0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0f08c",
   "metadata": {},
   "source": [
    "### Count Vectorizer Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7190eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Logistic Regression Model\n",
    "def predict_cv_lr(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41d1bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5209a958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Japanese\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese       Japanese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0005b",
   "metadata": {},
   "source": [
    "The only prediction that is inaccurate is Chinese, since the function predicted Japanese instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2fbd2",
   "metadata": {},
   "source": [
    "### Count Vectorizer Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae871a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Naive Bayes Model\n",
    "def predict_cv_nb(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44022085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86fc6f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese        Arabic\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e4762",
   "metadata": {},
   "source": [
    "This time, the only prediction that is inaccurate is Japanese. It predicts it as Arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1963f",
   "metadata": {},
   "source": [
    "### Tf-idf Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcbffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_lr(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86923c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "146a2957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese       Chinese\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2144c",
   "metadata": {},
   "source": [
    "This model predicts all but Japanese correctly. This time it predicts it as Chinese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb771280",
   "metadata": {},
   "source": [
    "### Tf-idf Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0acf901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_nb(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e4d401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f64c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese          Thai\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa6785",
   "metadata": {},
   "source": [
    "Again, all but Japanese is predicted correctly. It predicts Thai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a701ff9",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dd8aa",
   "metadata": {},
   "source": [
    "Unfortunately none of the four models predicted the text correctly 100% of the time. The next step would be to figure out why and find a way so that the model predicts correctly 100% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d22a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained Word2Vec model\n",
    "nlp = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15633a7",
   "metadata": {},
   "source": [
    "Now we are going to build a model using Word2Vec on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data\n",
    "X_w2v = X.apply(simple_preprocess)\n",
    "\n",
    "# Instantiate the model\n",
    "w2v = Word2Vec(window=5, min_count=2, workers=4)\n",
    "\n",
    "# Create a vocabulary\n",
    "w2v.build_vocab(X_w2v, progress_per=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "w2v.train(X_w2v, total_examples=w2v.corpus_count, epochs=w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b9c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v.wv['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b273f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_w2v_train, X_w2v_test, y_train, y_test = train_test_split(w2v.wv, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict the labels of different texts\n",
    "def predict(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict the labels of different texts\n",
    "def predict(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    print(lang[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff179e",
   "metadata": {},
   "source": [
    "### Word2Vec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4854dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model on top of Word2Vec\n",
    "w2v_lr = LogisticRegression()\n",
    "w2v_lr.fit(X_w2v_train, y_train)\n",
    "y_w2v_lr_pred = w2v_lr.predict(X_w2v_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
