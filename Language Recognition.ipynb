{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e7fa5c",
   "metadata": {},
   "source": [
    "# Language Recognition and First Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874a318",
   "metadata": {},
   "source": [
    "The goal of this project is two-fold, to predict one of 22 different languages based on its text as input, and to predict whether or not a sentence is a first sentence in a paragraph. \n",
    "\n",
    "I aim to solve the text classification problem by creating four different models: Logistic Regression and Naive Bayes implementations with each model incorporating Count Vectorizer and Tf-idf to pre-process the data. I will then use trained models to make predictions on some texts gathered from the Internet and evaluate their performance.\n",
    "\n",
    "In order to solve the first sentence prediction problem, I will first focus on just one language, which will be Chinese. I will use spaCy to split each paragraph of text into sentneces. I will then label each sentence with a 1 or 0 indicating whether the sentence is first in the paragraph or not. Then I will use latent semantic analysis to transfor the data into document vectors. Lastly, I will apply Naive Bayes to the pre-processed data to make the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c96202",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ece9db",
   "metadata": {},
   "source": [
    "I will start by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae724d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import spacy\n",
    "import jieba\n",
    "# import nagisa\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dffa6",
   "metadata": {},
   "source": [
    "## Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323491",
   "metadata": {},
   "source": [
    "### Import and display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48ae73",
   "metadata": {},
   "source": [
    "This data was taken from the Kaggle language identification data set (https://www.kaggle.com/datasets/zarajamshaid/language-identification-datasst). The data was taken from WiLi-2018 wikipedia dataset, which contains 235,000 paragraphs of 235 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca0f4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsutinalar i̇ngilizce tsuutina kanadada albert...</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>müller mox figura centralis circulorum doctoru...</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>برقی بار electric charge تمام زیرجوہری ذرات کی...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "5  エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運...  Japanese\n",
       "6  tsutinalar i̇ngilizce tsuutina kanadada albert...   Turkish\n",
       "7  müller mox figura centralis circulorum doctoru...     Latin\n",
       "8  برقی بار electric charge تمام زیرجوہری ذرات کی...      Urdu\n",
       "9  シャーリー・フィールドは、サン・ベルナルド・アベニュー沿い市民センターとrtマーティン高校に...  Japanese"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and display the data\n",
    "df = pd.read_csv('language.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78271284",
   "metadata": {},
   "source": [
    "The data contains two columns, one is natrual language text and the other appears to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ece79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the shape of the data.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc76cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estonian      1000\n",
       "Swedish       1000\n",
       "English       1000\n",
       "Russian       1000\n",
       "Romanian      1000\n",
       "Persian       1000\n",
       "Pushto        1000\n",
       "Spanish       1000\n",
       "Hindi         1000\n",
       "Korean        1000\n",
       "Chinese       1000\n",
       "French        1000\n",
       "Portugese     1000\n",
       "Indonesian    1000\n",
       "Urdu          1000\n",
       "Latin         1000\n",
       "Turkish       1000\n",
       "Japanese      1000\n",
       "Dutch         1000\n",
       "Tamil         1000\n",
       "Thai          1000\n",
       "Arabic        1000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the data in more detail.\n",
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03598ace",
   "metadata": {},
   "source": [
    "Based on the initial inspection of the data we see it consists of 1,000 examples each of 22 languages. This is plenty of data for my purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538937b0",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b885e",
   "metadata": {},
   "source": [
    "Next, I will create the X and y variables for modeling. Since the y variable will be the same for all models, I will only need to create it once. However, since I am using two different methods to create two sets of feature vectors, I need to create two different Xs. I will create each X variable as I go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857af1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = df['Text']\n",
    "y = df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bdcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels into numbers.\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f0d44",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizer + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d534b",
   "metadata": {},
   "source": [
    "First I will create a Count Vectorizer Logistic Regression model. The first step is to create some sparse matrices for the X variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce62345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Count Vectorizer and set X_cv equal to the transformed data\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7d1f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6f0fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the number of tokens in the first example.\n",
    "len(set(X[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa82bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(X_cv, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0038b8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17600, 277720), (4400, 277720))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the shape of the data\n",
    "X_cv_train.shape, X_cv_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aaa6612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of CountVectorizer\n",
    "cv_lr = LogisticRegression(max_iter = 10000, C = 0.1)\n",
    "cv_lr.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9be89",
   "metadata": {},
   "source": [
    "Next, I will make a dictionary of language-label encoded numbers so that I will be able to understand the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a0383e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 17, 19, 18,  2,  8, 20, 10, 21,  7, 12,  5,  1,  9,  6, 16, 13,\n",
       "       11, 14, 15,  3,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b83a985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c7d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Arabic\n",
      "1 Chinese\n",
      "2 Dutch\n",
      "3 English\n",
      "4 Estonian\n",
      "5 French\n",
      "6 Hindi\n",
      "7 Indonesian\n",
      "8 Japanese\n",
      "9 Korean\n",
      "10 Latin\n",
      "11 Persian\n",
      "12 Portugese\n",
      "13 Pushto\n",
      "14 Romanian\n",
      "15 Russian\n",
      "16 Spanish\n",
      "17 Swedish\n",
      "18 Tamil\n",
      "19 Thai\n",
      "20 Turkish\n",
      "21 Urdu\n"
     ]
    }
   ],
   "source": [
    "d = dict(zip([ 4, 17, 19, 18,  2,  8, 20, 10, 21,  7, 12,  5,  1,  9,  6, 16, 13,\n",
    "       11, 14, 15,  3,  0],['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
    "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
    "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
    "       'Romanian', 'Russian', 'English', 'Arabic']))\n",
    "\n",
    "for k, v in sorted(d.items()): \n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3846f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9932954545454545\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       798\n",
      "           1       0.97      0.99      0.98       802\n",
      "           2       1.00      1.00      1.00       795\n",
      "           3       0.96      1.00      0.98       801\n",
      "           4       1.00      0.98      0.99       794\n",
      "           5       0.99      0.99      0.99       797\n",
      "           6       1.00      1.00      1.00       786\n",
      "           7       1.00      0.99      1.00       820\n",
      "           8       0.95      0.99      0.97       796\n",
      "           9       1.00      1.00      1.00       791\n",
      "          10       0.99      0.99      0.99       805\n",
      "          11       1.00      1.00      1.00       807\n",
      "          12       1.00      0.99      1.00       811\n",
      "          13       1.00      0.99      1.00       795\n",
      "          14       1.00      1.00      1.00       811\n",
      "          15       1.00      0.99      1.00       799\n",
      "          16       1.00      0.99      0.99       814\n",
      "          17       1.00      1.00      1.00       785\n",
      "          18       1.00      0.99      1.00       802\n",
      "          19       1.00      0.99      1.00       799\n",
      "          20       1.00      0.99      1.00       799\n",
      "          21       1.00      0.99      1.00       793\n",
      "\n",
      "    accuracy                           0.99     17600\n",
      "   macro avg       0.99      0.99      0.99     17600\n",
      "weighted avg       0.99      0.99      0.99     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_lr_train_pred = cv_lr.predict(X_cv_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_cv_train, y_cv_lr_train_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_cv_train, y_cv_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9feaf4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.9397727272727273\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       202\n",
      "           1       0.77      0.53      0.63       198\n",
      "           2       1.00      0.99      1.00       205\n",
      "           3       0.91      0.96      0.93       199\n",
      "           4       0.98      0.93      0.95       206\n",
      "           5       0.99      0.99      0.99       203\n",
      "           6       1.00      0.96      0.98       214\n",
      "           7       1.00      0.96      0.98       180\n",
      "           8       0.51      0.94      0.66       204\n",
      "           9       1.00      0.91      0.95       209\n",
      "          10       0.94      0.95      0.95       195\n",
      "          11       1.00      0.98      0.99       193\n",
      "          12       0.98      0.94      0.96       189\n",
      "          13       1.00      0.97      0.99       205\n",
      "          14       0.99      0.97      0.98       189\n",
      "          15       0.98      0.93      0.95       201\n",
      "          16       1.00      0.98      0.99       186\n",
      "          17       1.00      0.97      0.99       215\n",
      "          18       1.00      0.99      0.99       198\n",
      "          19       1.00      0.95      0.97       201\n",
      "          20       1.00      0.96      0.98       201\n",
      "          21       1.00      0.98      0.99       207\n",
      "\n",
      "    accuracy                           0.94      4400\n",
      "   macro avg       0.96      0.94      0.94      4400\n",
      "weighted avg       0.96      0.94      0.94      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_lr_test_pred = cv_lr.predict(X_cv_test)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_cv_test, y_cv_lr_test_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_cv_test, y_cv_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9727675",
   "metadata": {},
   "source": [
    "With 99% accuracy on the test data and 94% accuracy on the test data, we have a fairly accurate model on our hands that generalizes well. Not bad for a first attempt. F1 score for Chinese and Japanese are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39a21a",
   "metadata": {},
   "source": [
    "### Model 2: CountVectorizer + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4adc4",
   "metadata": {},
   "source": [
    "Now I will create a CountVectorizer with Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7b2d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of CountVectorizer\n",
    "cv_nb = MultinomialNB(alpha=0.1)\n",
    "cv_nb.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0454eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9910227272727272\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       798\n",
      "           1       1.00      0.99      0.99       802\n",
      "           2       1.00      1.00      1.00       795\n",
      "           3       0.85      1.00      0.92       801\n",
      "           4       1.00      0.99      1.00       794\n",
      "           5       0.98      0.99      0.99       797\n",
      "           6       1.00      0.98      0.99       786\n",
      "           7       1.00      0.99      0.99       820\n",
      "           8       1.00      1.00      1.00       796\n",
      "           9       1.00      1.00      1.00       791\n",
      "          10       1.00      0.98      0.99       805\n",
      "          11       1.00      1.00      1.00       807\n",
      "          12       1.00      0.99      1.00       811\n",
      "          13       1.00      0.96      0.98       795\n",
      "          14       1.00      0.99      1.00       811\n",
      "          15       1.00      0.99      1.00       799\n",
      "          16       1.00      0.99      0.99       814\n",
      "          17       1.00      1.00      1.00       785\n",
      "          18       1.00      0.99      1.00       802\n",
      "          19       1.00      0.99      0.99       799\n",
      "          20       1.00      0.99      1.00       799\n",
      "          21       1.00      0.99      0.99       793\n",
      "\n",
      "    accuracy                           0.99     17600\n",
      "   macro avg       0.99      0.99      0.99     17600\n",
      "weighted avg       0.99      0.99      0.99     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_cv_nb_train_pred = cv_nb.predict(X_cv_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_cv_train, y_cv_nb_train_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_cv_train, y_cv_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58640f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9593181818181818\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       202\n",
      "           1       0.95      0.54      0.69       198\n",
      "           2       0.99      1.00      0.99       205\n",
      "           3       0.75      1.00      0.86       199\n",
      "           4       1.00      0.96      0.98       206\n",
      "           5       0.98      1.00      0.99       203\n",
      "           6       1.00      0.99      0.99       214\n",
      "           7       0.99      0.97      0.98       180\n",
      "           8       0.67      0.92      0.78       204\n",
      "           9       1.00      0.97      0.99       209\n",
      "          10       0.99      0.93      0.96       195\n",
      "          11       1.00      0.99      1.00       193\n",
      "          12       1.00      0.94      0.97       189\n",
      "          13       1.00      0.98      0.99       205\n",
      "          14       1.00      0.98      0.99       189\n",
      "          15       0.98      1.00      0.99       201\n",
      "          16       1.00      0.99      1.00       186\n",
      "          17       1.00      1.00      1.00       215\n",
      "          18       1.00      0.99      1.00       198\n",
      "          19       1.00      0.99      0.99       201\n",
      "          20       1.00      0.99      0.99       201\n",
      "          21       1.00      0.99      0.99       207\n",
      "\n",
      "    accuracy                           0.96      4400\n",
      "   macro avg       0.97      0.96      0.96      4400\n",
      "weighted avg       0.97      0.96      0.96      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_cv_nb_test_pred = cv_nb.predict(X_cv_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_cv_test, y_cv_nb_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_cv_test, y_cv_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3ce2d",
   "metadata": {},
   "source": [
    "With 99% accuracy on the training data and 95% accuracy on the test data, this model is slightly better than the first model. F1 score for Chinese and Japanese are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c406be",
   "metadata": {},
   "source": [
    "### Model 3: Tf-idf + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b98e22",
   "metadata": {},
   "source": [
    "Now I will build the third model, Tf-idf with Logistic Regression. First I need to transform the text using the Tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdeabd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 277720)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Tf-idf vectorizer and set X_tf equal to the transformed data\n",
    "tf = TfidfVectorizer()\n",
    "X_tf = tf.fit_transform(X)\n",
    "\n",
    "#Examine the shape of the new vectors.\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "990c99c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x277720 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first element.\n",
    "X_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d26701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_tf_train, X_tf_test, y_tf_train, y_tf_test = train_test_split(X_tf, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1880fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Logistic Regression model on top of Tf-idf\n",
    "tf_lr = LogisticRegression(max_iter = 10000, C = 0.1)\n",
    "tf_lr.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "356f6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9740340909090909\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       806\n",
      "           1       0.83      0.98      0.90       812\n",
      "           2       1.00      0.97      0.99       805\n",
      "           3       0.79      0.99      0.88       803\n",
      "           4       0.99      0.95      0.97       801\n",
      "           5       0.97      0.99      0.98       778\n",
      "           6       1.00      0.98      0.99       793\n",
      "           7       1.00      0.97      0.99       791\n",
      "           8       1.00      0.94      0.97       787\n",
      "           9       1.00      1.00      1.00       809\n",
      "          10       0.98      0.93      0.95       796\n",
      "          11       1.00      0.98      0.99       801\n",
      "          12       0.99      0.95      0.97       801\n",
      "          13       1.00      0.95      0.97       804\n",
      "          14       1.00      0.98      0.99       830\n",
      "          15       0.99      0.99      0.99       790\n",
      "          16       1.00      0.96      0.98       802\n",
      "          17       1.00      0.99      1.00       784\n",
      "          18       1.00      0.99      0.99       810\n",
      "          19       1.00      0.99      0.99       806\n",
      "          20       1.00      0.98      0.99       793\n",
      "          21       1.00      0.98      0.99       798\n",
      "\n",
      "    accuracy                           0.97     17600\n",
      "   macro avg       0.98      0.97      0.97     17600\n",
      "weighted avg       0.98      0.97      0.97     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_lr_train_pred = tf_lr.predict(X_tf_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_train, y_tf_lr_train_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_train, y_tf_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25e6390e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9413636363636364\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       194\n",
      "           1       0.50      0.98      0.66       188\n",
      "           2       1.00      0.97      0.99       195\n",
      "           3       0.78      0.97      0.86       197\n",
      "           4       0.99      0.98      0.99       199\n",
      "           5       0.96      0.99      0.98       222\n",
      "           6       1.00      0.97      0.99       207\n",
      "           7       1.00      0.98      0.99       209\n",
      "           8       1.00      0.39      0.56       213\n",
      "           9       1.00      0.96      0.98       191\n",
      "          10       0.98      0.90      0.94       204\n",
      "          11       1.00      0.98      0.99       199\n",
      "          12       0.98      0.93      0.95       199\n",
      "          13       1.00      0.94      0.97       196\n",
      "          14       0.99      0.99      0.99       170\n",
      "          15       1.00      0.96      0.98       210\n",
      "          16       1.00      0.96      0.98       198\n",
      "          17       1.00      0.99      0.99       216\n",
      "          18       1.00      0.98      0.99       190\n",
      "          19       1.00      0.98      0.99       194\n",
      "          20       1.00      0.97      0.99       207\n",
      "          21       1.00      0.98      0.99       202\n",
      "\n",
      "    accuracy                           0.94      4400\n",
      "   macro avg       0.96      0.94      0.94      4400\n",
      "weighted avg       0.96      0.94      0.94      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_lr_test_pred = tf_lr.predict(X_tf_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_test, y_tf_lr_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_test, y_tf_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b417fa",
   "metadata": {},
   "source": [
    "With 97% accuracy on the training data and 94% accuracy on the test data, this is a pretty good model. F1 score for Chinese and Japanese are significantly lower than the rest of the data. However, it is surprising that overall it didn't do as well as the Count Vectorizer models did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be581f",
   "metadata": {},
   "source": [
    "### Model 4: Tf-idf + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d876b4",
   "metadata": {},
   "source": [
    "I will now create the fourth model: Tf-idf with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "901be54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Naive Bayes model on top of Tf-idf\n",
    "tf_nb = MultinomialNB()\n",
    "tf_nb.fit(X_tf_train, y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "081914a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9839204545454545\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      0.96      0.98       812\n",
      "           2       1.00      0.99      0.99       805\n",
      "           3       0.77      1.00      0.87       803\n",
      "           4       1.00      0.98      0.99       801\n",
      "           5       0.97      0.99      0.98       778\n",
      "           6       1.00      0.98      0.99       793\n",
      "           7       1.00      0.98      0.99       791\n",
      "           8       1.00      0.97      0.99       787\n",
      "           9       1.00      1.00      1.00       809\n",
      "          10       1.00      0.95      0.97       796\n",
      "          11       1.00      1.00      1.00       801\n",
      "          12       1.00      0.97      0.98       801\n",
      "          13       1.00      0.96      0.98       804\n",
      "          14       1.00      0.99      0.99       830\n",
      "          15       0.99      0.99      0.99       790\n",
      "          16       0.99      0.99      0.99       802\n",
      "          17       1.00      1.00      1.00       784\n",
      "          18       1.00      0.99      1.00       810\n",
      "          19       1.00      0.99      0.99       806\n",
      "          20       1.00      0.99      1.00       793\n",
      "          21       1.00      0.99      0.99       798\n",
      "\n",
      "    accuracy                           0.98     17600\n",
      "   macro avg       0.99      0.98      0.98     17600\n",
      "weighted avg       0.99      0.98      0.98     17600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_tf_nb_train_pred = tf_nb.predict(X_tf_train)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_train, y_tf_nb_train_pred))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_tf_train, y_tf_nb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "528b0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.9395454545454546\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       194\n",
      "           1       0.93      0.54      0.68       188\n",
      "           2       0.98      0.98      0.98       195\n",
      "           3       0.66      0.99      0.80       197\n",
      "           4       0.99      0.98      0.98       199\n",
      "           5       0.94      0.99      0.97       222\n",
      "           6       1.00      0.97      0.98       207\n",
      "           7       0.98      0.98      0.98       209\n",
      "           8       0.99      0.57      0.72       213\n",
      "           9       1.00      0.97      0.98       191\n",
      "          10       0.98      0.91      0.94       204\n",
      "          11       1.00      1.00      1.00       199\n",
      "          12       0.98      0.94      0.96       199\n",
      "          13       1.00      0.95      0.98       196\n",
      "          14       0.59      0.99      0.74       170\n",
      "          15       1.00      0.98      0.99       210\n",
      "          16       0.97      0.98      0.98       198\n",
      "          17       0.99      1.00      0.99       216\n",
      "          18       1.00      0.99      0.99       190\n",
      "          19       0.99      0.98      0.99       194\n",
      "          20       0.99      0.99      0.99       207\n",
      "          21       1.00      0.99      0.99       202\n",
      "\n",
      "    accuracy                           0.94      4400\n",
      "   macro avg       0.95      0.94      0.94      4400\n",
      "weighted avg       0.96      0.94      0.94      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_tf_nb_test_pred = tf_nb.predict(X_tf_test)\n",
    "print('Accuracy score:') \n",
    "print(accuracy_score(y_tf_test, y_tf_nb_test_pred))\n",
    "print('Classification report:') \n",
    "print(classification_report(y_tf_test, y_tf_nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7645c",
   "metadata": {},
   "source": [
    "With an accuracy score of 98% on the training data and 95% on the test data this is a good model. F1 score for Chinese, Japanese, and Hindi are significantly lower than the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ecad91",
   "metadata": {},
   "source": [
    "### Hyperparameter Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87449e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data Accuracy</th>\n",
       "      <th>Test Data Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.993295</td>\n",
       "      <td>0.939773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.991023</td>\n",
       "      <td>0.959318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.974034</td>\n",
       "      <td>0.941364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.983920</td>\n",
       "      <td>0.939545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Training Data Accuracy  \\\n",
       "Count Vectorizer Logistic Regression                0.993295   \n",
       "                 Naive Bayes                        0.991023   \n",
       "Tf-idf           Logistic Regression                0.974034   \n",
       "                 Naive Bayes                        0.983920   \n",
       "\n",
       "                                      Test Data Accuracy  \n",
       "Count Vectorizer Logistic Regression            0.939773  \n",
       "                 Naive Bayes                    0.959318  \n",
       "Tf-idf           Logistic Regression            0.941364  \n",
       "                 Naive Bayes                    0.939545  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies of the four models for training and test data.\n",
    "data = [[accuracy_score(y_cv_train, y_cv_lr_train_pred), accuracy_score(y_cv_test, y_cv_lr_test_pred)], \n",
    "        [accuracy_score(y_cv_train, y_cv_nb_train_pred), accuracy_score(y_cv_test, y_cv_nb_test_pred)],\n",
    "        [accuracy_score(y_tf_train, y_tf_lr_train_pred), accuracy_score(y_tf_test, y_tf_lr_test_pred)], \n",
    "        [accuracy_score(y_tf_train, y_tf_nb_train_pred), accuracy_score(y_tf_test, y_tf_nb_test_pred)]\n",
    "       ]\n",
    "\n",
    "accuracy_df = pd.DataFrame(data, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Logistic Regression', 'Naive Bayes', 'Logistic Regression','Naive Bayes']],\n",
    "                          columns = ['Training Data Accuracy','Test Data Accuracy'])\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6b29f",
   "metadata": {},
   "source": [
    "From the data we can see that in terms of accuracy, the best model is Count Vectorizer with Naive Bayes. I wonder why a Count Vectorizer model is better than Tf-idf. Perhaps it is because we only need to know whether a word is of a particular language, and how many times it occurs in a document is not important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc6668",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775194c",
   "metadata": {},
   "source": [
    "In the next section, I will use the models I have created to predict the langauge of a given paragraph of text. The text has been gathered from different websites in various languages. In total, there are 22 paragraphs, one for each langauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a73e70f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8a1dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for each langauge\n",
    "# Estonian. Source: https://uueduudised.ee/uudis/eesti/ekre-ettepanek-homofilmifestivali-raha-ukraina-kultuuriseltsile-anda-ei-leidnud-rakveres-toetust/\n",
    "a = \"Rakvere linnavolikokku kuuluvates Eesti Konservatiivse Rahvaerakonna saadikutes tekitas küsimusi homofilmifestivali Festheart rahastamine ajal, mil Ukrainas käib sõda ja selle asemel võiks linna eelarves homopropagandale eraldatava kultuurirahaga toetada pigem Ukraina kultuuriseltsi.\"\n",
    "\n",
    "# Swedish. Source: https://www.svt.se/sport/ishockey/mallost-efter-forsta-perioden-i-odesmatchen\n",
    "b = \"Grabbarna känns verkligen laddade för uppgiften, men det är 40 långa minuter kvar, sa Djurgårdens Sebastian Strandberg i C Mores sändning efter de första 20 minuterna. Halvvägs in i ångestmatchen tog Timrå ledningen med 1-0 genom Robin Hanzl, som styrde in matchens första mål, innan Ty Rattie, 56 sekunder senare, utökade till 2-0. Hanzl blev också tvåmålsskytt när Djurgården gav bort pucken i egen zon och släppte in ett tredje mål.\"\n",
    "\n",
    "# Thai. Source: https://nlovecooking.com/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3/%E0%B8%AA%E0%B8%B9%E0%B8%95%E0%B8%A3%E0%B8%AD%E0%B8%B2%E0%B8%AB%E0%B8%B2%E0%B8%A3%E0%B9%84%E0%B8%97%E0%B8%A2-2/\n",
    "c = \"คุณค่าของอาหารไทยด้านวัฒนธรรม การถ่ายทอดความรู้ด้านการทำอาหารใน อาหารไทย นั้น แสดงถึงภูมิปัญญาของคนไทย และ วัฒนธรรมด้านอาหารของคนไทย บ่งบอกถึงความเจริญของชนชาตินั้นๆ อาหารไทย มีเอกลักษณ์ที่แตกต่างจากอาหารของชนชาติอื่นๆ สามารถปรับปรุงรสชาติให้เข้ากับคนุกชาติได้ จึงแสดงถึงคุณค่าของอาหารไทย ที่ทำให้คนทั่วโลกยอมรับ\"\n",
    "\n",
    "# Tamil. Source: https://artsandculture.google.com/entity/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AE%B0%E0%AF%8D-%E0%AE%B5%E0%AE%B0%E0%AE%B2%E0%AE%BE%E0%AE%B1%E0%AF%81/g11cls_rl0p?hl=ta\n",
    "d = \"தமிழர் மத்திய ஆசியா, வட இந்தியா நிலப்பரப்புகளில் இருந்து காலப்போக்கில் தென் இந்தியா வந்தனர் என்பது மற்றைய கருதுகோள். எப்படி இருப்பினும் தமிழர் இனம் தொன்மையான மக்கள் இனங்களில் ஒன்று. தமிழர்களின் தோற்றம் மற்ற திராவிடர்களைப் போலவே இன்னும் தெளிவாக அறியப்படவில்லை.\"\n",
    "\n",
    "# Dutch. Source: https://www.stuivengalederwaren.nl/leukste-hollandse-tassen/\n",
    "e = \"Berba staat vooral bekend om de zachte leren tassen en bijpassende portemonnees. En met de vele vakjes en een lange schouderbanden sluiten de tassen én portemonnees perfect aan bij de wensen van de Hollandse vrouw (en man!). Zo heb je met Berba dé ideale combinatie van schoonheid en functionaliteit.\"\n",
    "\n",
    "# Japanese. Source: https://twitter.com/twitterjp/status/923671036758958080\n",
    "f = \"いつも、そして何年もの間、Twitterをご利用いただきありがとうございます。おかげさまで日本での月間利用者数が4500万を超えました。安心してサービスをご利用いただけますように、一層の努力を行います。引き続きのご指導、ご支援のほど、よろしくお願い申し上げます\"\n",
    "\n",
    "# Turkish. Source: https://www.haberturk.com/seren-serengil-e-annesi-nevin-serengil-den-isyan-3396288-magazin\n",
    "g = \"Kimi varlıkla imtihan edilir, kimi yoklukla... Kimi hastalıkla imtihan edilir, kimi sağlıkla... Ama evlatla imtihan edilmek imtihanların en zorudur. Çünkü canını yakan yine kendi canındır. Bin parçaya da bölünürsün ama yine de nefret edemezsin. Rabbim hiç kimseyi evlatlarıyla imtihan etmesin.\"\n",
    "\n",
    "# Latin. Source: https://www.lipsum.com/\n",
    "h = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "# Urdu. Source: https://www.urdunews.com/node/658036\n",
    "i = \"انہوں نے کہا کہ پاکستان میں رجسٹر اور غیر رجسٹرڈ افغان مہاجرین کی تعداد 40 لاکھ کے لگ بھگ ہے۔آرمی چیف نے مشرقی سرحد کی صورتحال پر کہا کہ لائن آف کنٹرول پر حالات بہتر ہیں اور وہاں بسنے والے شہریوں کی زندگی میں امن آیا ہے۔ان کا کہنا تھا کہ انڈین سپر سونک میزائل کے پاکستان میں گرنے کا واقعہ انتہائی تشویشناک ہے۔ عالمی برادری اس کا نوٹس لے گی کیونکہ اس سے یہاں عام شہریوں کا جانی نقصان بھی ہو سکتا تھا جبکہ اس میزائل کے راستے میں آنے والا کوئی مسافر طیارہ بھی نشانہ بن سکتا تھا۔\"\n",
    "\n",
    "# Indonesian. Source: https://news.detik.com/berita/d-6013602/ingat-13-lokasi-di-jakarta-ditutup-jelang-sahur-pukul-0100-0500-wib\n",
    "j = \"Filterisasi mengantisipasi sahur on the road atau SOTR dilakukan Polda Metro Jaya di wilayah DKI Jakarta selama bulan Ramadan. Perlu diingat, total ada 13 lokasi yang diberlakukan filterisasi pada jam-jam menjelang sahur.\"\n",
    "\n",
    "# Protugese. Source: https://www.dn.pt/internacional/ucrania-acusa-tropas-russas-de-abrirem-fogo-contra-manifestantes-pacificos-14737367.html\n",
    "k = \"'Hoje em Energodar, os moradores da cidade reuniram-se de novo manifestando-se em apoio da Ucrânia e cantando o hino nacional', postou na rede social Facebook a responsável pelos Direitos Humanos no Parlamento ucraniano, Lyoudmyla Denisova.\"\n",
    "\n",
    "# French. Source: https://www.francetvinfo.fr/elections/presidentielle/presidentielle-2022-ces-12-millions-de-francais-encore-indecis_5059294.html\n",
    "l = \"Le 10 avril se tiendra le premier tour de l'élection présidentielle. Vendredi 1er avril, 37 % des électeurs ne savent toujours pas pour qui ils vont voter. Ces indécis sont des personnes qui sont certaines d'aller voter, mais qui peuvent changer d'avis. Un citoyen hésite ainsi entre Yannick Jadot (EELV) et Emmanuel Macron (LREM). Une autre dit se laisser encore quelques jours pour consulter les programmes. Près de 6 sur 10 électeurs de Yannick Jadot, Anne Hidalgo (PS) et Fabien Roussel (PCF) sont indécis.\"\n",
    "\n",
    "# Chinese. Source: https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD\n",
    "m = \"1949年，以毛泽东主席为领袖的中国共产党领导中国人民解放军在内战中取得优势，实际控制中国大陆，同年10月1日宣布建立中华人民共和国以及中央人民政府，与迁至台湾地区的中华民国政府形成至今的台海现状格局。中华人民共和国成立初期遵循和平共处五项原则的外交政策，1971年在联合国取得了原属于中华民国的中国代表权及其联合国安理会常任理事国席位，并陆续加入部分联合国其他专门机构。而后广泛参与例如国际奥委会、亚太经合组织、二十国集团、世界贸易组织等重要国际组织，并成为上海合作组织、金砖国家、一带一路、亚洲基础设施投资银行、区域全面经济伙伴关系协定等国际合作组织项目的发起国和创始国。据皮尤研究中心的调查，随着国际影响力的增强，中华人民共和国已被许多国家、组织视为世界经济的重要支柱与潜在超级大国之一[41][42][43]。\"\n",
    "\n",
    "# Korean. Source: https://news.kbs.co.kr/news/view.do?ncd=5430540\n",
    "n = \"호남 출신인 한 전 총리는 경제 관료 출신으로, 김대중 정부에서 청와대 경제수석, 노무현 정부에서 국무총리를 지냈고, 이명박 정부에서 주미 대사를, 박근혜 정부에서는 무역협회장을 역임했습니다. 가장 중요한 건 경제라던 윤 당선인은 한 전 총리에 대해 '통합형 총리'에 맞고, 외교와 통상, 경제 전문가로서의 경륜을 높이 사고 있다고 말한 것으로 전해졌습니다. 2007년 총리 후보자로 국회 인사청문회를 통과했던 만큼, 민주당이 다수인 국회에서의 임명 동의 등 여러 측면을 고려한 인선이란 분석도 나옵니다.\"\n",
    "\n",
    "# Hindi. Source: https://www.bbc.com/hindi/india-60964637\n",
    "o = \"भारत दौरे पर आए नेपाल के प्रधानमंत्री शेर बहादुर देउबा की शनिवार को प्रधानमंत्री नरेंद्र मोदी समेत कई महत्वपूर्ण नेताओं से मुलाकात हुई. साथ ही भारत और नेपाल ने शनिवार को सीमा पार रेलवे नेटवर्क समेत कई विकास परियोजनाओं का उद्घाटन किया. इस मौके पर नेपाल के प्रधानमंत्री शेर बहादुर देउबा ने कहा कि दोनों देशों के बीच चल रहे सीमा विवाद को सुलझाने के लिए कोई साझा व्यवस्था बने.\"\n",
    "\n",
    "# Spanish. Source: https://cnnespanol.cnn.com/2022/04/02/analisis-putin-esta-cometiendo-los-mismos-errores-que-condenaron-a-hitler-trax/\n",
    "p = \"Pero los tanques rusos se han visto obstaculizados por otra razón sorprendente: la falta de combustible. La falta de combustible es parte de un problema mayor. El ejército ruso, del que alguna vez se alardeó se ha estancado en Ucrania no solo por la feroz resistencia, sino por algo más prosaico: la logística.\"\n",
    "\n",
    "# Pushto. Source: https://www.bbc.com/pashto/world-60909321\n",
    "q = \"ملګري ملتونه وايي نژدې دوه ميلیونه اوکرايني ماشومان اوس د روسيې له بمبارۍ ګاونډیو هېوادونو ته تښتېدلي دي. يونيسېف او د بشري مرستو نورو ټولنو خبرداری ورکړی، دا ماشومان یې له خپلو ميندو او نورو ښځينه اوکراينيو کډوالو سره د قاچاق او ناوړه ګټې اخيستو لوړې کچې خطر سره مخامخ دي.\"\n",
    "\n",
    "# Persian. Source: https://www.bbc.com/persian/afghanistan-60966238\n",
    "r = \"گزارش‌های قبلا به نقل از طالبان طالبان منتشر شده بود که این گروه برای آزادی مارک فرریکس خواستار رهایی یک افغان به نام بشیر نورزی شده بوده است که در حال گذراندن محکومیت حبس ابد به جرم قاچاق مواد مخدر در ایالات متحده است.\"\n",
    "\n",
    "# Romanian. Source: https://www.digi24.ro/stiri/externe/sua-trimite-ucrainei-echipament-de-protectie-in-caz-de-atacuri-chimice-zelenski-rusii-planuiesc-atacuri-puternice-in-donbas-si-harkov-1891921\n",
    "s = \"Președintele ucrainean Volodimir Zelenski spune că retragerea trupelor rusești din nordul țării este „înceată dar vizibilă”. Acesta avertizează însă ucrainenii că vor urma „lupte grele” în estul țării, în zonele Donbas și Harkov. Peste 3.000 de oameni au reușit să părăsească orașul-port Mariupol, mai spune președintele Ucrainei. Între timp, SUA ajută țara pentru posibile atacuri chimice, trimițând echipament personal de protecție. De asemenea, Pentagonul va oferi Ucrainei un ajutor militar suplimentar de până la 300 de milioane de dolari.\"\n",
    "\n",
    "# Russian. Source: https://ria.ru/20220402/protesty-1781464774.html\n",
    "t = \"Таким образом, расходы британцев на энергию вырастут в среднем на 700 фунтов в год и составят около двух тысяч. Из-за этого годовая инфляция в феврале достигла в Британии рекордного за 30 лет уровня — 6,2 процента.\"\n",
    "\n",
    "# English. Source: https://www.wsj.com/articles/tesla-deliveries-rose-in-quarter-elon-musk-calls-exceptionally-difficult-11648917258?mod=hp_lead_pos2\n",
    "u = \"Tesla Inc. vehicle deliveries rose in the first quarter, but missed Wall Street expectations as the company struggled with global supply-chain disruptions and a brief Covid-19 shutdown at its Shanghai factory. This was an *exceptionally* difficult quarter due to supply chain interruptions & China zero Covid policy,” Tesla Chief Executive Elon Musk tweeted Saturday morning. Tesla employees and key suppliers 'saved the day,' he added.\"\n",
    "\n",
    "# Arabic.\n",
    "v = \"وقال المتحدث باسم الوزارة أحمد الصحاف، إن 'الوزير فؤاد حسين استقبل اليوم سفراء مجموعة G7 المعتمدين لدى العراق، واستعرض تفاصيل وأبعاد زيارته المرتقبة إلى موسكو ووارسو ضمن مجموعة الاتصال العربية على المستوى وزارء في جامعة الدول العربية لمتابعة وإجراء المشاورات والاتصالات اللازمة مع الأطراف المعنية بالأزمة الروسية-الأوكرانية بهدف المساهمة في إيجاد الحلول الدبلوماسية للازمة وإنهاء الحرب القائم'.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cfa0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0f08c",
   "metadata": {},
   "source": [
    "### Count Vectorizer Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7190eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Logistic Regression Model\n",
    "def predict_cv_lr(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41d1bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5209a958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Japanese\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese       Japanese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0005b",
   "metadata": {},
   "source": [
    "The only prediction that is inaccurate is Chinese, since the function predicted Japanese instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2fbd2",
   "metadata": {},
   "source": [
    "### Count Vectorizer Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae871a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Count Vectorizer with Naive Bayes Model\n",
    "def predict_cv_nb(text):\n",
    "    x = cv.transform([text])\n",
    "    lang = cv_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44022085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_cv_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86fc6f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese    Indonesian\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e4762",
   "metadata": {},
   "source": [
    "This time, Japanese is predicted as Russian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1963f",
   "metadata": {},
   "source": [
    "### Tf-idf Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcbffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_lr(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_lr.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86923c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_lr(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "146a2957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese       Chinese\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2144c",
   "metadata": {},
   "source": [
    "This model predicts all but Chinese correctly. This time it predicts it as Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb771280",
   "metadata": {},
   "source": [
    "### Tf-idf Naive Bayes Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0acf901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict languages using the Tf-idf with Logistic Regression Model\n",
    "def predict_tf_nb(text):\n",
    "    x = tf.transform([text])\n",
    "    lang = tf_nb.predict(x)\n",
    "    lang = le.inverse_transform(lang)\n",
    "    return lang[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e4d401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "for char in languages:\n",
    "    predictions.append(predict_tf_nb(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f64c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Estonian</th>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thai</th>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latin</th>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urdu</th>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesian</th>\n",
       "      <td>Indonesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugese</th>\n",
       "      <td>Portugese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>Korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pushto</th>\n",
       "      <td>Pushto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Persian</th>\n",
       "      <td>Persian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions\n",
       "Estonian      Estonian\n",
       "Swedish        Swedish\n",
       "Thai              Thai\n",
       "Tamil            Tamil\n",
       "Dutch            Dutch\n",
       "Japanese      Romanian\n",
       "Turkish        Turkish\n",
       "Latin            Latin\n",
       "Urdu              Urdu\n",
       "Indonesian  Indonesian\n",
       "Portugese    Portugese\n",
       "French          French\n",
       "Chinese        Chinese\n",
       "Korean          Korean\n",
       "Hindi            Hindi\n",
       "Spanish        Spanish\n",
       "Pushto          Pushto\n",
       "Persian        Persian\n",
       "Romanian      Romanian\n",
       "Russian        Russian\n",
       "English        English\n",
       "Arabic          Arabic"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to view the results.\n",
    "predictions_df = pd.DataFrame(predictions, \n",
    "                           index = [list(df['language'].unique())],\n",
    "                          columns = ['Predictions'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa6785",
   "metadata": {},
   "source": [
    "All but Japanese and Chinese are predicted correctly. These are predicted as Hindi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cee79",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dd8aa",
   "metadata": {},
   "source": [
    "Based on the precision and recall scores I have from the trained models, along with the fact that Japanese and Chinese texts are difficult to predict, makes me think that Chinese and Japanese are not being properly vectorized since there are no spaces between words in these languages.\n",
    "\n",
    "In a future project, I could find a way to properly vectorize Chinese and Japanese data and combine them into one vector along with the other langauges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fedcba",
   "metadata": {},
   "source": [
    "## Predicting the First Sentence of a Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003c3ff",
   "metadata": {},
   "source": [
    "Now I will build a model to predict whether a sentence is the first in a paragraph or not.\n",
    "\n",
    "First, we need to create a new dataset which contains only one langauge. In this case, I will use Chinese since it is one of the only langauges in the datasets which still contains punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9be651f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>年月日，參與了「snh第三屆年度金曲大賞best 」。月日，出演由优酷视频，盟将威影视，嗨乐...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>在他们出发之前，罗伯特·菲茨罗伊送给了达尔文一卷查尔斯·赖尔所著《地质学原理》（在南美他得到...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>系列的第一款作品《薩爾達傳說》（ゼルダの伝説）在年月日於日本發行，之後在年內於美國和歐洲地區...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>历史上的柔远驿是为了给琉球贡使及随员提供食宿之所，同时它也成为中琉间商业和文化交流的枢纽。琉...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text language\n",
       "13   胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...  Chinese\n",
       "110  年月日，參與了「snh第三屆年度金曲大賞best 」。月日，出演由优酷视频，盟将威影视，嗨乐...  Chinese\n",
       "122  在他们出发之前，罗伯特·菲茨罗伊送给了达尔文一卷查尔斯·赖尔所著《地质学原理》（在南美他得到...  Chinese\n",
       "151  系列的第一款作品《薩爾達傳說》（ゼルダの伝説）在年月日於日本發行，之後在年內於美國和歐洲地區...  Chinese\n",
       "227  历史上的柔远驿是为了给琉球贡使及随员提供食宿之所，同时它也成为中琉间商业和文化交流的枢纽。琉...  Chinese"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn = df[df['language'] == 'Chinese']\n",
    "cn.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98faeb86",
   "metadata": {},
   "source": [
    "Before proceeding any further, I will do the train-test split on the data so that I am not splitting over paragraphs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61ef7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data.\n",
    "X_cn_train, X_cn_test, y_cn_train, y_cn_test = train_test_split(cn['Text'], cn['language'], test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdbcce",
   "metadata": {},
   "source": [
    "Next, I will use spaCy to split the paragraphs up into individual sentneces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fb8edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate spacy model\n",
    "nlp = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f3fc5",
   "metadata": {},
   "source": [
    "Now I will create a function which can create a new dataframe out of the original dataframe. The new dataframe will consist of sentences taken from the paragraphs, and each sentence will be labeled with a 1 or 0, representing being the first sentence in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01c23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the function that will label sentences as a first sentence or not.\n",
    "def first_sent(sentences, sent):\n",
    "    if sent == sentences[0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Now create the function which takes in a dataframe and creates a new dataframe.\n",
    "def new_df(df):\n",
    "    \n",
    "    # Create a list containing all of the spacy doc objects, one for each paragraph.\n",
    "    docs = []\n",
    "    for i in range(df.shape[0]):\n",
    "        doc = nlp(df.iloc[i])\n",
    "        docs.append(doc) \n",
    "\n",
    "    # Create a list containing all of the lists of each paragraph's sentences.\n",
    "    paragraphs = []\n",
    "    for doc in docs:\n",
    "        sentences = list(doc.sents)\n",
    "        paragraphs.append(sentences)\n",
    "\n",
    "    # Build a dictionary that will contain all of the sentences across all \n",
    "    # paragraphs and label whether each entry is the first sentence in the paragraph or not.      \n",
    "    sentences_dict = [{'Sentence':str(sent),'First':first_sent(sentences, sent)} for sentences in paragraphs for sent in sentences]\n",
    "\n",
    "    # Create a dataframe from the dictionary.\n",
    "    new_df = pd.DataFrame(sentences_dict)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7dccb",
   "metadata": {},
   "source": [
    "Now I will use the function to create new training and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83122381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>First</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>现今的柔远驿于年修复，并辟为福州对外友好关系史博物馆，馆址在原址大门西侧，门牌号为福州市台江...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>建筑为坐北朝南向，大门后有插屏，其后为天井、两侧是披榭。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>厅堂的主建筑面阔三间，进深五柱，为穿斗式杉木结构的双层楼房，馆周围用封火墙围绕，占地面积约平...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>前后天井用传统的假山盆景装点，后天井还放置着搜集到的数十方葬在福州的古代琉球人的墓碑。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>苏珊娜死后查尔斯的几个姐妹接管了家中事务，而罗伯特出诊归来后更是对庄园实行严厉的统治。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>年月，查尔斯和哥哥伊拉斯谟进入了什鲁斯伯里学校，那里的校长塞缪尔·巴特勒在当地名望颇高。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>古典文化是学校的主要教学内容，查尔斯厌恶拉丁语和古希腊语，但还是能够应付那些死记硬背的学习，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>快毕业时，查尔斯受哥哥影响迷上了化学，阅读了威廉·亨利的《化学问答》等化学书籍。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>他们在自家花园里做化学实验，同学们还给达尔文取了个“瓦斯”的绰号。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>岁的达尔文爱上了狩猎，常常参与韦奇伍德家的射击活动。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  First\n",
       "0  现今的柔远驿于年修复，并辟为福州对外友好关系史博物馆，馆址在原址大门西侧，门牌号为福州市台江...      1\n",
       "1                       建筑为坐北朝南向，大门后有插屏，其后为天井、两侧是披榭。      0\n",
       "2  厅堂的主建筑面阔三间，进深五柱，为穿斗式杉木结构的双层楼房，馆周围用封火墙围绕，占地面积约平...      0\n",
       "3        前后天井用传统的假山盆景装点，后天井还放置着搜集到的数十方葬在福州的古代琉球人的墓碑。      0\n",
       "4        苏珊娜死后查尔斯的几个姐妹接管了家中事务，而罗伯特出诊归来后更是对庄园实行严厉的统治。      1\n",
       "5       年月，查尔斯和哥哥伊拉斯谟进入了什鲁斯伯里学校，那里的校长塞缪尔·巴特勒在当地名望颇高。      0\n",
       "6  古典文化是学校的主要教学内容，查尔斯厌恶拉丁语和古希腊语，但还是能够应付那些死记硬背的学习，...      0\n",
       "7           快毕业时，查尔斯受哥哥影响迷上了化学，阅读了威廉·亨利的《化学问答》等化学书籍。      0\n",
       "8                  他们在自家花园里做化学实验，同学们还给达尔文取了个“瓦斯”的绰号。      0\n",
       "9                         岁的达尔文爱上了狩猎，常常参与韦奇伍德家的射击活动。      0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_train_new = new_df(X_cn_train)\n",
    "cn_train_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3589797f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>First</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在高中聯招時被狠狠淘汰，只剩私立高中與五專這兩條路可以抉擇，為了再也不要考聯考，下定決心選擇...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>個性孤僻的她，在進入五專後，讓她只是每天過著上學、放學的規律生活，生活在自己的狹小空間。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>直到五專四年級時，遇見一位鄉音很重的國文老師，這才使張曼娟封閉的世界打開一道入口，得以灑進溫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>因為這位國文老師在發作文時，她的作文成為壓卷，更重要的是稱讚她是一朵奇葩。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿米尔被这一秘密震惊，之后出发前往喀布尔。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>在一个阿富汗出租车司机法里德（farid）的帮助下寻找索拉博。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>法里德是抗击苏联入侵时的阿富汗老兵，起初对阿米尔怀有敌意，但了解了阿米尔前往喀布尔真正的目的...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>二人了解到塔利班军官经常到孤儿院，给院长一些钱之后带走一个孩子，索拉博就已经被首領带走。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>院长告诉阿米尔可以在足球赛上找到那名军官，阿米尔和法里德二人于是前往足球场，目睹了那名首領执...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>公路從馬蹄灣起沿豪灣的海岸綫北延公里至獅子灣、公里至不列顛尼亞灘、公里至豪灣盡頭的史戈密殊、...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  First\n",
       "0  在高中聯招時被狠狠淘汰，只剩私立高中與五專這兩條路可以抉擇，為了再也不要考聯考，下定決心選擇...      1\n",
       "1       個性孤僻的她，在進入五專後，讓她只是每天過著上學、放學的規律生活，生活在自己的狹小空間。      0\n",
       "2  直到五專四年級時，遇見一位鄉音很重的國文老師，這才使張曼娟封閉的世界打開一道入口，得以灑進溫...      0\n",
       "3              因為這位國文老師在發作文時，她的作文成為壓卷，更重要的是稱讚她是一朵奇葩。      0\n",
       "4                              阿米尔被这一秘密震惊，之后出发前往喀布尔。      1\n",
       "5                    在一个阿富汗出租车司机法里德（farid）的帮助下寻找索拉博。      0\n",
       "6  法里德是抗击苏联入侵时的阿富汗老兵，起初对阿米尔怀有敌意，但了解了阿米尔前往喀布尔真正的目的...      0\n",
       "7       二人了解到塔利班军官经常到孤儿院，给院长一些钱之后带走一个孩子，索拉博就已经被首領带走。      0\n",
       "8  院长告诉阿米尔可以在足球赛上找到那名军官，阿米尔和法里德二人于是前往足球场，目睹了那名首領执...      0\n",
       "9  公路從馬蹄灣起沿豪灣的海岸綫北延公里至獅子灣、公里至不列顛尼亞灘、公里至豪灣盡頭的史戈密殊、...      1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_test_new = new_df(X_cn_test)\n",
    "cn_test_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cf8c6",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2e31b",
   "metadata": {},
   "source": [
    "Now, I will check the distribution of values in the 'First' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50c030dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3151, 1: 800})\n",
      "Counter({0: 781, 1: 200})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(cn_train_new['First']))\n",
    "print(Counter(cn_test_new['First']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc47669",
   "metadata": {},
   "source": [
    "It looks like we have an imbalanced dataset. I will use random over sampler to oversample the minority class (the first sentences) for the test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f93d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 3151, 0: 3151})\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the random over sampler \n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Resample X, y\n",
    "X_ros_train, y_ros_train = ros.fit_resample(cn_train_new['Sentence'].values.reshape(-1,1), cn_train_new['First'].values.reshape(-1,1))\n",
    "\n",
    "# Check new value distribution \n",
    "print(Counter(y_ros_train))\n",
    "\n",
    "# Reshape the new samples\n",
    "X_ros_train = X_ros_train.flatten()\n",
    "y_ros_train = y_ros_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16554d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 781, 0: 781})\n"
     ]
    }
   ],
   "source": [
    "# Resample X, y\n",
    "X_ros_test, y_ros_test = ros.fit_resample(cn_test_new['Sentence'].values.reshape(-1,1), cn_test_new['First'].values.reshape(-1,1))\n",
    "\n",
    "# Check new value distribution \n",
    "print(Counter(y_ros_test))\n",
    "\n",
    "# Reshape the new samples\n",
    "X_ros_test = X_ros_test.flatten()\n",
    "y_ros_test = y_ros_test.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9256e6b",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d855b",
   "metadata": {},
   "source": [
    "Now that I have a new, self supervised dataset that has had the minority class oversampled, I can perform a latent semantic analysis on the data to create document vectors, which I can then train a model on in order to make predictions about whether a sentence is the first in the paragraph or not.\n",
    "\n",
    "I will first try this using Count Vectorizer and then try it using Tf-idf and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9087b6",
   "metadata": {},
   "source": [
    "First, I will create a function that can tokenize Chinese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d251dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Chinese text tokenizer\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "stop_words = ['。', '，']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5ac90",
   "metadata": {},
   "source": [
    "### LSA Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24787039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/bn/55fdvsy52pl19l11rfd0vm340000gn/T/jieba.cache\n",
      "Loading model cost 0.433 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a document term matrix using Count Vectorizer and fit it using the training data.\n",
    "cv_train = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "cv_train_matrix = cv_train.fit_transform(X_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7694410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrix into latent semantic analysis.\n",
    "svd_cv_train = TruncatedSVD(n_components=75)\n",
    "lsa_cv_train = svd_cv_train.fit_transform(cv_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50df6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test data.\n",
    "cv_test = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "cv_test_matrix = cv_test.fit_transform(X_ros_test)\n",
    "svd_cv_test = TruncatedSVD(n_components=75)\n",
    "lsa_cv_test = svd_cv_test.fit_transform(cv_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573c776",
   "metadata": {},
   "source": [
    "Now, I will use a Logistic Regression model to train a model that takes as inputs the latent semnatic analysis and predicts whether or not a sentence is the first in the paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aec8c550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model.\n",
    "lsa_lr = LogisticRegression()\n",
    "lsa_lr.fit(lsa_cv_train, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25348d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.6734370041256744\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70      3151\n",
      "           1       0.71      0.60      0.65      3151\n",
      "\n",
      "    accuracy                           0.67      6302\n",
      "   macro avg       0.68      0.67      0.67      6302\n",
      "weighted avg       0.68      0.67      0.67      6302\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2369  782]\n",
      " [1276 1875]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_lr_train_pred = lsa_lr.predict(lsa_cv_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_lr_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_lr_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55529b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.5332906530089628\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.69      0.60       781\n",
      "           1       0.55      0.37      0.44       781\n",
      "\n",
      "    accuracy                           0.53      1562\n",
      "   macro avg       0.54      0.53      0.52      1562\n",
      "weighted avg       0.54      0.53      0.52      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[542 239]\n",
      " [490 291]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_lr_test_pred = lsa_lr.predict(lsa_cv_test)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_lr_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_lr_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd8003",
   "metadata": {},
   "source": [
    "When generalizing to the test data, F1 scores are 0.64 and 0.39 for non-first sentences and first sentences respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b07cce",
   "metadata": {},
   "source": [
    "### LSA Using Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "354df80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document term matrix using Tf-idf and fit and transform it using the training data.\n",
    "tf_train = TfidfVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "tf_train_matrix = tf_train.fit_transform(X_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d3cc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to turn document term matrix into latent semantic analysis.\n",
    "svd_tf_train = TruncatedSVD(n_components=75)\n",
    "lsa_tf_train = svd_tf_train.fit_transform(tf_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51ec6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test data\n",
    "tf_test = TfidfVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
    "tf_test_matrix = tf_test.fit_transform(X_ros_test)\n",
    "svd_tf_test = TruncatedSVD(n_components=75)\n",
    "lsa_tf_test = svd_tf_test.fit_transform(tf_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97edb3",
   "metadata": {},
   "source": [
    "Now, I will use a Logistic Regression model to train a model that takes as inputs the latent semnatic analysis and predicts whether or not a sentence is the first in the paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a63e8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model.\n",
    "lsa_tf = LogisticRegression()\n",
    "lsa_tf.fit(lsa_tf_train, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d57b93bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.695176134560457\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.71      3151\n",
      "           1       0.71      0.66      0.68      3151\n",
      "\n",
      "    accuracy                           0.70      6302\n",
      "   macro avg       0.70      0.70      0.69      6302\n",
      "weighted avg       0.70      0.70      0.69      6302\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2301  850]\n",
      " [1071 2080]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "y_lsa_tf_train_pred = lsa_tf.predict(lsa_tf_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, y_lsa_tf_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, y_lsa_tf_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, y_lsa_tf_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ccaec9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.5921895006402048\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.69      0.63       781\n",
      "           1       0.62      0.49      0.55       781\n",
      "\n",
      "    accuracy                           0.59      1562\n",
      "   macro avg       0.60      0.59      0.59      1562\n",
      "weighted avg       0.60      0.59      0.59      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[540 241]\n",
      " [396 385]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "y_lsa_tf_test_pred = lsa_tf.predict(lsa_tf_test)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, y_lsa_tf_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, y_lsa_tf_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, y_lsa_tf_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f31d86",
   "metadata": {},
   "source": [
    "When generalizing to the test data, F1 scores are 0.61 and 0.59 for non-first sentences and first sentences respectively. This is significanly better than using CountVectorizer, so using Tf-idf really makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fe0a7",
   "metadata": {},
   "source": [
    "### Optimizing for F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a51346",
   "metadata": {},
   "source": [
    "For the last part of this project, I will train a model that optimizes for F1 score instead of accuracy to see if the F1 score can be improved any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f9cc6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(), param_grid={'C': [1]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Logistic Regression Model using the Tf-idf LSA \n",
    "# training data by using GridSearch and optimizing for f1 score.\n",
    "lsa_tf_f1 = LogisticRegression()\n",
    "gs = GridSearchCV(lsa_tf_f1, param_grid={'C':[1]}, scoring='f1')\n",
    "gs.fit(lsa_tf_train, y_ros_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08bbeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.695176134560457\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.71      3151\n",
      "           1       0.71      0.66      0.68      3151\n",
      "\n",
      "    accuracy                           0.70      6302\n",
      "   macro avg       0.70      0.70      0.69      6302\n",
      "weighted avg       0.70      0.70      0.69      6302\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2301  850]\n",
      " [1071 2080]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the training data\n",
    "gs_train_pred = gs.predict(lsa_tf_train)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_train, gs_train_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_train, gs_train_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_train, gs_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ac0cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.5921895006402048\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.69      0.63       781\n",
      "           1       0.62      0.49      0.55       781\n",
      "\n",
      "    accuracy                           0.59      1562\n",
      "   macro avg       0.60      0.59      0.59      1562\n",
      "weighted avg       0.60      0.59      0.59      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[540 241]\n",
      " [396 385]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate the model using the test data\n",
    "gs_test_pred = gs.predict(lsa_tf_test)\n",
    "print('Accuracy Score:') \n",
    "print(accuracy_score(y_ros_test, gs_test_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_ros_test, gs_test_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_ros_test, gs_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0b541",
   "metadata": {},
   "source": [
    "The F1 score doesn't change when I optimize for F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1b2b2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd30e33",
   "metadata": {},
   "source": [
    "The best model for predicting whether a sentence is the first in its paragraph was Logistic Regression using Tf-idf with LSA. It didn't make a difference whether I optimized for F1 score or just left it as optimizing for accuracy.\n",
    "\n",
    "Below are the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7dc5f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Training Data</th>\n",
       "      <th>Test Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.673437</td>\n",
       "      <td>0.533291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.645661</td>\n",
       "      <td>0.443936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tf-idf</th>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.695176</td>\n",
       "      <td>0.592190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.684098</td>\n",
       "      <td>0.547264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Training Data  Test Data\n",
       "Count Vectorizer Accuracy       0.673437   0.533291\n",
       "                 F1 Score       0.645661   0.443936\n",
       "Tf-idf           Accuracy       0.695176   0.592190\n",
       "                 F1 Score       0.684098   0.547264"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table containing the accuracies and f1 scores for training and test data.\n",
    "data1 = [[accuracy_score(y_ros_train, y_lsa_lr_train_pred), accuracy_score(y_ros_test, y_lsa_lr_test_pred)], \n",
    "        [f1_score(y_ros_train, y_lsa_lr_train_pred), f1_score(y_ros_test, y_lsa_lr_test_pred)],\n",
    "    [accuracy_score(y_ros_train, gs_train_pred), accuracy_score(y_ros_test, gs_test_pred)], \n",
    "        [f1_score(y_ros_train, gs_train_pred), f1_score(y_ros_test, gs_test_pred)]\n",
    "       ]\n",
    "\n",
    "accuracy_f1 = pd.DataFrame(data1, \n",
    "                           index = [['Count Vectorizer', 'Count Vectorizer','Tf-idf','Tf-idf'],\n",
    "                                    ['Accuracy', 'F1 Score', 'Accuracy', 'F1 Score']],\n",
    "                          columns = ['Training Data','Test Data'])\n",
    "accuracy_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7950c2e",
   "metadata": {},
   "source": [
    "Given that the challenge was to predict whether a sentence was first in a paragraph or not, that I found a model which could predict this with 60% accuracy is pretty good. This shows that the Tf-idf vectorizer and latent semantic analysis were able to derive enough meaning from the data so that a Logistic Regression estimator could make predictions with a decent level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e747b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'accuracy_df' (DataFrame)\n",
      "Stored 'accuracy_f1' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Store variables for use in other notebooks\n",
    "%store accuracy_df\n",
    "%store accuracy_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
